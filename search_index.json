[["index.html", "Welcome to my portfolio 1 My portfolio 1.1 introduction 1.2 My curriculum vitae", " Welcome to my portfolio Lamyae el Mahboud 2023-06-06 1 My portfolio 1.1 introduction Welcome to my portofolio. My name is Lamyae el Mahboud and I’m studying at the Utrecht University of Applied Sciences. In this github page you’ll find different skills related to data science and coding in R. The version of R that has been used to make this bookdown page is: R version 4.2.3 (2023-03-15 ucrt) 1.2 My curriculum vitae To get to know me a little better, I’ll show you my CV I made with Rstudio. To make this CV in Rstudio, I used a template made by Mitchell O’Hara-Wild and Rob Hyndman "],["c.-elegans-plate-experiment-analysis.html", "2 C. elegans plate experiment analysis 2.1 Introduction 2.2 analyzing plan", " 2 C. elegans plate experiment analysis 2.1 Introduction In this analyse we’ll read and analyze a data file. While analyzing the data file, we come across a few issues we try to fix. The data we’re gonna use in this exercise was obtained from an experiment in which adult C.elegans were exposed to varying concentrations of different compounds. The variables RawData (the outcome - number of offspring counted as an integer value, after incubation time), compName (the generic name of the compound/chemical), the compConcentration (the concentration of the compound), and the expType are the most important variables in this dataset. After we reviewed the Excel File. We see in the Excel file that there is no data in the colomns ‘plateRow’ en ‘plateColmn’. Some colomns are not necessary like ‘expDate’, ‘expResearcher’, ‘expTime’, ‘éxpUnit’. This kind of information is metadata and should not be in your raw data file. It makes the table cluttered and information less easy to find. Some data in some colomns are the same for all conditions, like ‘compDelivery’ and ‘bacterialStrain’. This kind of information could be mentioned in your metadata to make your data file more readable and less chaotic. after reviewing the file we want to open the excel file in R using the {readxl} package. library(tidyverse) #load in tidyverse package library(readxl) #load in readxl package data &lt;- read_excel(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) #load in the data that we want to use for this assignment Before we go ahead an start analyzing our data. We’re gonna inspect the following colomns: “Rawdata”, “compName” and “compConcentration” head(data[c(&#39;RawData&#39;, &#39;compName&#39;, &#39;compConcentration&#39;)]) #we want to see the first few lines of the colomns RawData compName and compConcentration ## # A tibble: 6 × 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 The colomn ‘Rawdata’ is a dbl type colomn, this is what we expected. The colomn ‘compName’ is a “chr” type, this is what we expected . The colomn ‘compConcentration’ is a “chr” type, this is not correct since they are only numbers in the colomn. We’re gonna make a scatterplot of the dataset. As you can see in the code below we put the “compConcentration” on the x-axis, the “Rawdata” counts on the y-axis. For each level in “compName” we assign a colour and for each level in “expType” we assign a different symbol. library(tidyverse) #load in the tidyverse data ggplot(data = data, aes(x = compConcentration, y = RawData )) + geom_point(aes(color = compName, shape = expType)) + labs(title = &quot;Rawdata of each compName and their expType&quot;, y = &quot;Rawdata&quot;, x = &quot;compConcentration(nM)&quot;) + theme_minimal() #scatterplot for the different compounds and the varying concentrations in nM We just made a scatterplot with the data from the excel file we imported earlier. Each compName has its own color and each expType has its own shape, but we see one big problem. As we can see is the x-axis not very readable, actually not at all! when we imported the Excel file, the colomn ‘compConcentration’ had a charachter type. Because of this every single charachter in each cell was shown on the x-axis and the numbers weren’t rounded. This explains why the x-axis wasn’t readable. To correct this mistake, we’re gonna change the type of the colomn “compConcentration” from “chr” to “dbl”. data$compConcentration &lt;- as.numeric(data$compConcentration) #change the type of the colomn from &lt;chr&gt; to &lt;dbl&gt; library(tidyverse) #load in tidyverse package ggplot(data = data, aes(x = log10(compConcentration), y = RawData)) + geom_jitter(aes(color = compName, shape = expType), size = 3,alpha = 0.8) + labs(title = &quot;Rawdata of each compName and their expType&quot;, y = &quot;Rawdata&quot;, x = &quot;compConcentration(nM)&quot;) + theme_minimal() #scatterplot of the data with fixed x-axis and jitter As we van see the x-axis is now readable. We see in the scatterplot that the positive control for this experiments is Ethanol. The negative control for this experiment is S-medium. if we really want to compare the different between the compound and experiments type. We need to normalize the data for the Negative control. We normalize the data by setting the mean value for the “controlNegative” to 1. All the other values are expressed as a fraction thereof. library(tidyverse) data_df &lt;- as.data.frame(data) controlnegative_mean &lt;- mean(data_df$RawData[data_df$expType == &quot;controlNegative&quot;]) #calculate mean of the control negative data_df$data_norm &lt;- data_df$RawData/controlnegative_mean ggplot(data = data_df, aes(x = log10(compConcentration), y = data_norm)) + geom_jitter(aes(color = compName, shape = expType), size = 1.5,alpha = 0.8, width = 0.2) + labs(title = &quot;Rawdata of each compName and their expType&quot;, y = &quot;Rawdata normalized&quot;, x = &quot;compConcentration(nM)&quot;) + theme_minimal() #scatterplot of the data with fixed x-axis and jitter When we look at the scatterplot we just made, we can that naphthalene gets lower when we rise the concentration. The same happens with decane and 2,6-diisopropylnaphthalene. To draw a conclusion we need to conduct further analysis. 2.2 analyzing plan If we want to analyze the data to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve(ic50), First we need to get the data and prepare it. We did this already as you can see above. after preparing the data we will perform the shapiro.test for every compound to check if the data (Rawdata) is normally distributed. All compounds need to have a p-value greater than 0.05. To test if there is a significant difference between compound concentrations we can use the one-sided t-test for each compound individually, but before we use the one sample t-test we need to check the variances in the group since the data is unpaired. To compare the IC50 of each compound we can use the {drc} package to visualize the different curves for each compound. "],["open-peer-review.html", "3 Open Peer Review 3.1 Introduction 3.2 Clarification 3.3 the second peer review", " 3 Open Peer Review 3.1 Introduction We’re gonna identify reproducibility issues in a scientific publication. We use the criteria for reproduciblity to see if our article is reproducible or not. The scientific article that we use can be found here. Table 3.1: Table1: The Transparency Criteria scores of the scientific article transparency Criteria Available Study Purpose Yes Data Availability Statement No Data Location No Study Location No Author Review Yes Ethics Statement No Funcing statement Yes Code Availability No 3.2 Clarification We’ve read the article and scored the article based on the Transparency Criteria in table 1. For each “Yes” in “Availability” the article scores one point. As we can see in table 1 the article scores four points out of eight. This article is not reproducible. The authors give us a statement why they started this research in the last paragraph. Although they show all their results, there is no raw data available of the western blot or the qPCR. The data of this study cant be accessed. There is no file or link with the raw or processed data available, only results. The paper doesn’t provide the exact study location in the methods section or else where in the paper. However it does give us a few laboratories where the research is done, like the Jackson Laboratory and the Bartek Laboratory. But since these laboratories have multiple locations we still don’t know where they performed the experiments. In the paper is an e-mail adress of one of the authors available. There are no contact information of the other authors. The paper doesn’t give us a statement within the paper indicating any ethical concerns. The paper does give us a statement within the paper indicating that the authors received funding for their research. In the last paragraph under Results and Discussion the authors give use a summary of all their findings. The codes that the researchers used in this experiment are not available for the reader. 3.3 the second peer review This paper is about morality as cooperation and minimizing the threath of infection during COVID-19 pandemic. We’re gonna review the Rcode the authors used and try to reproduce the data. The Rcode we’re analyzing asks: Do morak messages increse cooperation?. This code is analyzing two countries, the USA and India. It results in a image what is called “posterior_samples_by_country.RData”. which is a bayesian regression model. This code scores a two out of five on readability. Only the codes at th end contain a ‘header’, but it could be more informative. A header like “donation” doesn’t give the reader enough information of what is happening. before we could run the code, we had to download a dataset named “data.clean.txt”. library(rethinking) d&lt;-read.table(&quot;data.clean.txt&quot;,sep=&quot;\\t&quot;,header=T,stringsAsFactors = F) names(d) dat &lt;- list( C=d$CountC, A = standardize(d$Age), G = as.integer(as.factor(d$Gender)), #1 Man, 2 Woman CD = as.integer(d$COND), Mfm = standardize(d$MAC.fam), Mg = standardize(d$MAC.gro), Mr = standardize(d$MAC.rec), Mh = standardize(d$MAC.her), Md = standardize(d$MAC.def), Mfi = standardize(d$MAC.fai), Mp = standardize(d$MAC.pro), P = standardize(d$Precaution), S = standardize(d$Prosociality), D = standardize(d$donation), Dgr=standardize(d$danger.for.participant), CC = ifelse(d$COND==1,0,ifelse(d$COND==2,0,ifelse(d$COND==3,0,ifelse(d$COND==4,standardize(d$MAC.fam),ifelse(d$COND==5,standardize(d$MAC.gro),ifelse(d$COND==6,standardize(d$MAC.rec),ifelse(d$COND==7,standardize(d$MAC.her),ifelse(d$COND==8,standardize(d$MAC.def),ifelse(d$COND==9,standardize(d$MAC.fai),ifelse(d$COND==10,standardize(d$MAC.pro),NA)))))))))) #MAC dimension concordant with the condition ) datUSA &lt;- list( C = dat$C[d$Country==&quot;USA&quot;], A = dat$A[d$Country==&quot;USA&quot;], G = dat$G[d$Country==&quot;USA&quot;], #1 Man, 2 Woman CD = dat$CD[d$Country==&quot;USA&quot;], Mfm = dat$Mfm[d$Country==&quot;USA&quot;], Mg = dat$Mg[d$Country==&quot;USA&quot;], Mr = dat$Mr[d$Country==&quot;USA&quot;], Mh = dat$Mh[d$Country==&quot;USA&quot;], Md = dat$Md[d$Country==&quot;USA&quot;], Mfi = dat$Mfi[d$Country==&quot;USA&quot;], Mp = dat$Mp[d$Country==&quot;USA&quot;], P = dat$P[d$Country==&quot;USA&quot;], S = dat$S[d$Country==&quot;USA&quot;], D = dat$D[d$Country==&quot;USA&quot;], Dgr = dat$Dgr[d$Country==&quot;USA&quot;], CC = dat$CC[d$Country==&quot;USA&quot;]#MAC dimension concordant with the condition ) datIndia &lt;- list( C = dat$C[d$Country==&quot;India&quot;], A = dat$A[d$Country==&quot;India&quot;], G = dat$G[d$Country==&quot;India&quot;], #1 Man, 2 Woman CD = dat$CD[d$Country==&quot;India&quot;], Mfm = dat$Mfm[d$Country==&quot;India&quot;], Mg = dat$Mg[d$Country==&quot;India&quot;], Mr = dat$Mr[d$Country==&quot;India&quot;], Mh = dat$Mh[d$Country==&quot;India&quot;], Md = dat$Md[d$Country==&quot;India&quot;], Mfi = dat$Mfi[d$Country==&quot;India&quot;], Mp = dat$Mp[d$Country==&quot;India&quot;], P = dat$P[d$Country==&quot;India&quot;], S = dat$S[d$Country==&quot;India&quot;], D = dat$D[d$Country==&quot;India&quot;], Dgr = dat$Dgr[d$Country==&quot;India&quot;], CC = dat$CC[d$Country==&quot;India&quot;]#MAC dimension concordant with the condition ) set.seed(42) mUSA &lt;- ulam( alist( D ~ dnorm(muD,sigmaD), muD&lt;-aG[G]+bA*A+bP*P+bS*S+aC[CD]+bCon*CC+bFam*Mfm+bGro*Mg+bRec*Mr+bHer*Mh+bDef*Md+bFai*Mfi+bPro*Mp+bDg*Dgr, #Donation aG[G]~dnorm(0,0.2), bA~dnorm(0,0.5), bP~dnorm(0,0.5), bS~dnorm(0,0.5), aC[CD]~dnorm(0,0.2), bCon~dnorm(0,0.5), bFam~dnorm(0,0.5), bGro~dnorm(0,0.5), bRec~dnorm(0,0.5), bHer~dnorm(0,0.5), bDef~dnorm(0,0.5), bFai~dnorm(0,0.5), bPro~dnorm(0,0.5), bDg~dnorm(0,0.5), #Model of precaution and prosociality P ~ dnorm(muP,sigmaP), S ~ dnorm(muS,sigmaS), muP&lt;-aGP[G]+bAP*A+aCP[CD]+bConP*CC+bFamP*Mfm+bGroP*Mg+bRecP*Mr+bHerP*Mh+bDefP*Md+bFaiP*Mfi+bProP*Mp+bDgP*Dgr, muS&lt;-aGS[G]+bAS*A+aCS[CD]+bConS*CC+bFamS*Mfm+bGroS*Mg+bRecS*Mr+bHerS*Mh+bDefS*Md+bFaiS*Mfi+bProS*Mp+bDgS*Dgr, #Priors #Precaution aGP[G]~dnorm(0,0.2), bAP~dnorm(0,0.5), aCP[CD]~dnorm(0,0.2), bConP~dnorm(0,0.5), bFamP~dnorm(0,0.5), bGroP~dnorm(0,0.5), bRecP~dnorm(0,0.5), bHerP~dnorm(0,0.5), bDefP~dnorm(0,0.5), bFaiP~dnorm(0,0.5), bProP~dnorm(0,0.5), bDgP~dnorm(0,0.5), #ProSociality aGS[G]~dnorm(0,0.2), bAS~dnorm(0,0.5), aCS[CD]~dnorm(0,0.2), bConS~dnorm(0,0.5), bFamS~dnorm(0,0.5), bGroS~dnorm(0,0.5), bRecS~dnorm(0,0.5), bHerS~dnorm(0,0.5), bDefS~dnorm(0,0.5), bFaiS~dnorm(0,0.5), bProS~dnorm(0,0.5), bDgS~dnorm(0,0.5), #sigmas sigmaD~dexp(1), sigmaP~dexp(1), sigmaS~dexp(1), #Models of MAC dimensions Mfm ~ dnorm(mu_Fam,sigma_Fam), Mg ~ dnorm(mu_Gro,sigma_Gro), Mr ~ dnorm(mu_Rec,sigma_Rec), Mh ~ dnorm(mu_Her,sigma_Her), Md ~ dnorm(mu_Def,sigma_Def), Mfi ~ dnorm(mu_Fai,sigma_Fai), Mp ~ dnorm(mu_Pro,sigma_Pro), mu_Fam&lt;-aG_Fam[G]+bAge_Fam*A, mu_Gro&lt;-aG_Gro[G]+bAge_Gro*A, mu_Rec&lt;-aG_Rec[G]+bAge_Rec*A, mu_Her&lt;-aG_Her[G]+bAge_Her*A, mu_Def&lt;-aG_Def[G]+bAge_Def*A, mu_Fai&lt;-aG_Fai[G]+bAge_Fai*A, mu_Pro&lt;-aG_Pro[G]+bAge_Pro*A, #priors of MAC intercepts and slopes aG_Fam[G]~dnorm(0,0.2), aG_Gro[G]~dnorm(0,0.2), aG_Rec[G]~dnorm(0,0.2), aG_Her[G]~dnorm(0,0.2), aG_Def[G]~dnorm(0,0.2), aG_Fai[G]~dnorm(0,0.2), aG_Pro[G]~dnorm(0,0.2), bAge_Fam~dnorm(0,0.5), bAge_Gro~dnorm(0,0.5), bAge_Rec~dnorm(0,0.5), bAge_Her~dnorm(0,0.5), bAge_Def~dnorm(0,0.5), bAge_Fai~dnorm(0,0.5), bAge_Pro~dnorm(0,0.5), #sigmas sigma_Fam~dexp(1), sigma_Gro~dexp(1), sigma_Rec~dexp(1), sigma_Her~dexp(1), sigma_Def~dexp(1), sigma_Fai~dexp(1), sigma_Pro~dexp(1), #model of how dangerous COVID is perceived for the participant Dgr ~ dnorm(mu_Dang,sigma_Dang), mu_Dang&lt;-aG_Dang[G]+bAge_Dang*A, #priors aG_Dang[G]~dnorm(0,0.2), bAge_Dang~dnorm(0,0.5), sigma_Dang~dexp(1) ) , data=datUSA, chains=4 , cores=4 , log_lik=TRUE ,iter = 5000,control=list(max_treedepth=10,adapt_delta=0.95)) set.seed(42) mIndia &lt;- ulam( alist( D ~ dnorm(muD,sigmaD), muD&lt;-aG[G]+bA*A+bP*P+bS*S+aC[CD]+bCon*CC+bFam*Mfm+bGro*Mg+bRec*Mr+bHer*Mh+bDef*Md+bFai*Mfi+bPro*Mp+bDg*Dgr, #Donation aG[G]~dnorm(0,0.2), bA~dnorm(0,0.5), bP~dnorm(0,0.5), bS~dnorm(0,0.5), aC[CD]~dnorm(0,0.2), bCon~dnorm(0,0.5), bFam~dnorm(0,0.5), bGro~dnorm(0,0.5), bRec~dnorm(0,0.5), bHer~dnorm(0,0.5), bDef~dnorm(0,0.5), bFai~dnorm(0,0.5), bPro~dnorm(0,0.5), bDg~dnorm(0,0.5), #Model of precaution and prosociality P ~ dnorm(muP,sigmaP), S ~ dnorm(muS,sigmaS), muP&lt;-aGP[G]+bAP*A+aCP[CD]+bConP*CC+bFamP*Mfm+bGroP*Mg+bRecP*Mr+bHerP*Mh+bDefP*Md+bFaiP*Mfi+bProP*Mp+bDgP*Dgr, muS&lt;-aGS[G]+bAS*A+aCS[CD]+bConS*CC+bFamS*Mfm+bGroS*Mg+bRecS*Mr+bHerS*Mh+bDefS*Md+bFaiS*Mfi+bProS*Mp+bDgS*Dgr, #Priors #Precaution aGP[G]~dnorm(0,0.2), bAP~dnorm(0,0.5), aCP[CD]~dnorm(0,0.2), bConP~dnorm(0,0.5), bFamP~dnorm(0,0.5), bGroP~dnorm(0,0.5), bRecP~dnorm(0,0.5), bHerP~dnorm(0,0.5), bDefP~dnorm(0,0.5), bFaiP~dnorm(0,0.5), bProP~dnorm(0,0.5), bDgP~dnorm(0,0.5), #ProSociality aGS[G]~dnorm(0,0.2), bAS~dnorm(0,0.5), aCS[CD]~dnorm(0,0.2), bConS~dnorm(0,0.5), bFamS~dnorm(0,0.5), bGroS~dnorm(0,0.5), bRecS~dnorm(0,0.5), bHerS~dnorm(0,0.5), bDefS~dnorm(0,0.5), bFaiS~dnorm(0,0.5), bProS~dnorm(0,0.5), bDgS~dnorm(0,0.5), #sigmas sigmaD~dexp(1), sigmaP~dexp(1), sigmaS~dexp(1), #Models of MAC dimensions Mfm ~ dnorm(mu_Fam,sigma_Fam), Mg ~ dnorm(mu_Gro,sigma_Gro), Mr ~ dnorm(mu_Rec,sigma_Rec), Mh ~ dnorm(mu_Her,sigma_Her), Md ~ dnorm(mu_Def,sigma_Def), Mfi ~ dnorm(mu_Fai,sigma_Fai), Mp ~ dnorm(mu_Pro,sigma_Pro), mu_Fam&lt;-aG_Fam[G]+bAge_Fam*A, mu_Gro&lt;-aG_Gro[G]+bAge_Gro*A, mu_Rec&lt;-aG_Rec[G]+bAge_Rec*A, mu_Her&lt;-aG_Her[G]+bAge_Her*A, mu_Def&lt;-aG_Def[G]+bAge_Def*A, mu_Fai&lt;-aG_Fai[G]+bAge_Fai*A, mu_Pro&lt;-aG_Pro[G]+bAge_Pro*A, #priors of MAC intercepts and slopes aG_Fam[G]~dnorm(0,0.2), aG_Gro[G]~dnorm(0,0.2), aG_Rec[G]~dnorm(0,0.2), aG_Her[G]~dnorm(0,0.2), aG_Def[G]~dnorm(0,0.2), aG_Fai[G]~dnorm(0,0.2), aG_Pro[G]~dnorm(0,0.2), bAge_Fam~dnorm(0,0.5), bAge_Gro~dnorm(0,0.5), bAge_Rec~dnorm(0,0.5), bAge_Her~dnorm(0,0.5), bAge_Def~dnorm(0,0.5), bAge_Fai~dnorm(0,0.5), bAge_Pro~dnorm(0,0.5), #sigmas sigma_Fam~dexp(1), sigma_Gro~dexp(1), sigma_Rec~dexp(1), sigma_Her~dexp(1), sigma_Def~dexp(1), sigma_Fai~dexp(1), sigma_Pro~dexp(1), #model of how dangerous COVID is perceived for the participant Dgr ~ dnorm(mu_Dang,sigma_Dang), mu_Dang&lt;-aG_Dang[G]+bAge_Dang*A, #priors aG_Dang[G]~dnorm(0,0.2), bAge_Dang~dnorm(0,0.5), sigma_Dang~dexp(1) ) , data=datIndia, chains=4 , cores=4 , log_lik=TRUE ,iter = 5000,control=list(max_treedepth=10,adapt_delta=0.95)) #Sumarize the model precis(mUSA,depth=2) precis(mIndia,depth=2) #Sample posetrior and prior for graphical comparison postUSA&lt;-extract.samples(mUSA) postIndia&lt;-extract.samples(mIndia) save.image(file=&quot;posterior_samples_by_country.RData&quot;) While running the code we come across a few problems and bugs we need to fix. the first problem is that R didn’t recognize rethinking as a package, I found a person on Stackoverflow with the same problem and he recommended me this: install.packages(&quot;rethinking&quot;, repos=c(cran=&quot;https://cloud.r-project.org&quot;, rethinking=&quot;http://xcelab.net/R&quot;)) The second problem was that Rstudio couldn’t find the function “standerdize”. A search on google gave me this:. if(!require(&#39;robustHD&#39;)) { install.packages(&#39;robustHD&#39;) library(&#39;robustHD&#39;) } The next problem is that R didn’t recognize the function “ulam”. This is weird since it is part of the “rethinking” package. i tried fixing it with the code below, but R doesn’t recognize “rethinking” as a package. R cant recognize this package because there is no version of this package available for the version of R that’s been used for this analyse. I tried something else and i found a quick installion for the ‘rethinking’ package. This worked and the function “ulam” got recognized. devtools::install_github(&quot;stan-dev/cmdstanr&quot;) install.packages(c(&quot;coda&quot;,&quot;mvtnorm&quot;,&quot;devtools&quot;,&quot;loo&quot;,&quot;dagitty&quot;)) devtools::install_github(&quot;rmcelreath/rethinking&quot;) The next struggle we come across is that the CmdStan path has not been set. To fix this we have to give R the location of the CmdStan installion. To find the location we use the “cmdstan_default_install_path()” function. After i set the path with set_cmdstan_path, R couldn’t find the directory. Unfortunately, this problem i couln’t fix. cmdstan_default_install_path() set_cmdstan_path(&quot;../../../C:/Gebruikers/Lamya/OneDrive/Documenten/.cmdstan&quot;) To solve this problem i’ve tried multiple things. I’ve added “../” to go back to the “start”. I’ve changed “Users” to “Gebruikers”, since the laptop is in dutch. Unfornertly none of this could help it. With everything we could change this is the final outcome: library(rethinking) if(!require(&#39;robustHD&#39;)) { install.packages(&#39;robustHD&#39;) library(&#39;robustHD&#39;) } d&lt;-read.table(&quot;data.clean.txt&quot;,sep=&quot;\\t&quot;,header=T,stringsAsFactors = F) names(d) dat &lt;- list( C=d$CountC, A = standardize(d$Age), G = as.integer(as.factor(d$Gender)), #1 Man, 2 Woman CD = as.integer(d$COND), Mfm = standardize(d$MAC.fam), Mg = standardize(d$MAC.gro), Mr = standardize(d$MAC.rec), Mh = standardize(d$MAC.her), Md = standardize(d$MAC.def), Mfi = standardize(d$MAC.fai), Mp = standardize(d$MAC.pro), P = standardize(d$Precaution), S = standardize(d$Prosociality), D = standardize(d$donation), Dgr=standardize(d$danger.for.participant), CC = ifelse(d$COND==1,0,ifelse(d$COND==2,0,ifelse(d$COND==3,0,ifelse(d$COND==4,standardize(d$MAC.fam),ifelse(d$COND==5,standardize(d$MAC.gro),ifelse(d$COND==6,standardize(d$MAC.rec),ifelse(d$COND==7,standardize(d$MAC.her),ifelse(d$COND==8,standardize(d$MAC.def),ifelse(d$COND==9,standardize(d$MAC.fai),ifelse(d$COND==10,standardize(d$MAC.pro),NA)))))))))) #MAC dimension concordant with the condition ) set.seed(42) m1 &lt;- ulam( set_cmdstan_path(&quot;C:/Users/Lamya/OneDrive/Documenten/.cmdstan&quot;), alist( D ~ dnorm(muD,sigmaD), muD&lt;-aG[G]+bA*A+bP*P+bS*S+aC[CD]+bCon*CC+bFam*Mfm+bGro*Mg+bRec*Mr+bHer*Mh+bDef*Md+bFai*Mfi+bPro*Mp+bDg*Dgr, #Donation aG[G]~dnorm(0,0.2), bA~dnorm(0,0.5), bP~dnorm(0,0.5), bS~dnorm(0,0.5), aC[CD]~dnorm(0,0.2), bCon~dnorm(0,0.5), bFam~dnorm(0,0.5), bGro~dnorm(0,0.5), bRec~dnorm(0,0.5), bHer~dnorm(0,0.5), bDef~dnorm(0,0.5), bFai~dnorm(0,0.5), bPro~dnorm(0,0.5), bDg~dnorm(0,0.5), #Model of precaution and prosociality P ~ dnorm(muP,sigmaP), S ~ dnorm(muS,sigmaS), muP&lt;-aGP[G]+bAP*A+aCP[CD]+bConP*CC+bFamP*Mfm+bGroP*Mg+bRecP*Mr+bHerP*Mh+bDefP*Md+bFaiP*Mfi+bProP*Mp+bDgP*Dgr, muS&lt;-aGS[G]+bAS*A+aCS[CD]+bConS*CC+bFamS*Mfm+bGroS*Mg+bRecS*Mr+bHerS*Mh+bDefS*Md+bFaiS*Mfi+bProS*Mp+bDgS*Dgr, #Priors #Precaution aGP[G]~dnorm(0,0.2), bAP~dnorm(0,0.5), aCP[CD]~dnorm(0,0.2), bConP~dnorm(0,0.5), bFamP~dnorm(0,0.5), bGroP~dnorm(0,0.5), bRecP~dnorm(0,0.5), bHerP~dnorm(0,0.5), bDefP~dnorm(0,0.5), bFaiP~dnorm(0,0.5), bProP~dnorm(0,0.5), bDgP~dnorm(0,0.5), #ProSociality aGS[G]~dnorm(0,0.2), bAS~dnorm(0,0.5), aCS[CD]~dnorm(0,0.2), bConS~dnorm(0,0.5), bFamS~dnorm(0,0.5), bGroS~dnorm(0,0.5), bRecS~dnorm(0,0.5), bHerS~dnorm(0,0.5), bDefS~dnorm(0,0.5), bFaiS~dnorm(0,0.5), bProS~dnorm(0,0.5), bDgS~dnorm(0,0.5), #sigmas sigmaD~dexp(1), sigmaP~dexp(1), sigmaS~dexp(1), #Models of MAC dimensions Mfm ~ dnorm(mu_Fam,sigma_Fam), Mg ~ dnorm(mu_Gro,sigma_Gro), Mr ~ dnorm(mu_Rec,sigma_Rec), Mh ~ dnorm(mu_Her,sigma_Her), Md ~ dnorm(mu_Def,sigma_Def), Mfi ~ dnorm(mu_Fai,sigma_Fai), Mp ~ dnorm(mu_Pro,sigma_Pro), mu_Fam&lt;-aG_Fam[G]+bAge_Fam*A, mu_Gro&lt;-aG_Gro[G]+bAge_Gro*A, mu_Rec&lt;-aG_Rec[G]+bAge_Rec*A, mu_Her&lt;-aG_Her[G]+bAge_Her*A, mu_Def&lt;-aG_Def[G]+bAge_Def*A, mu_Fai&lt;-aG_Fai[G]+bAge_Fai*A, mu_Pro&lt;-aG_Pro[G]+bAge_Pro*A, #priors of MAC intercepts and slopes aG_Fam[G]~dnorm(0,0.2), aG_Gro[G]~dnorm(0,0.2), aG_Rec[G]~dnorm(0,0.2), aG_Her[G]~dnorm(0,0.2), aG_Def[G]~dnorm(0,0.2), aG_Fai[G]~dnorm(0,0.2), aG_Pro[G]~dnorm(0,0.2), bAge_Fam~dnorm(0,0.5), bAge_Gro~dnorm(0,0.5), bAge_Rec~dnorm(0,0.5), bAge_Her~dnorm(0,0.5), bAge_Def~dnorm(0,0.5), bAge_Fai~dnorm(0,0.5), bAge_Pro~dnorm(0,0.5), #sigmas sigma_Fam~dexp(1), sigma_Gro~dexp(1), sigma_Rec~dexp(1), sigma_Her~dexp(1), sigma_Def~dexp(1), sigma_Fai~dexp(1), sigma_Pro~dexp(1), #model of how dangerous COVID is perceived for the participant Dgr ~ dnorm(mu_Dang,sigma_Dang), mu_Dang&lt;-aG_Dang[G]+bAge_Dang*A, #priors aG_Dang[G]~dnorm(0,0.2), bAge_Dang~dnorm(0,0.5), sigma_Dang~dexp(1) ) , data=dat, chains=4 , cores=4 , log_lik=TRUE ,iter = 5000,control=list(max_treedepth=10,adapt_delta=0.95)) #Sumarize the model precis(m1,depth=2) #Sample posetrior and prior for graphical comparison post1&lt;-extract.samples(m1) set.seed(42) prio1&lt;-extract.prior(m1,n=10000) save.image(file=&quot;posterior_samples_single.RData&quot;) From a scale from 1 (very hard) to 5(very easy). This script gets a 1. It was very hard to read the code without any informative pseudo coding. It’s hard to see what the author wants to achieve with this code. Running the code wasn’t an easy job either. A lot of packages needed to be installed in a different way than what the author did. The reason could be that in the meantime the packages or RStudio got an update. To help this problem the authors could mention the version of the packages they used when running the code. In conclusion, this code is not really reproducible. "],["the-guerrila-structure.html", "4 The Guerrila structure 4.1 Why organizing data?", " 4 The Guerrila structure 4.1 Why organizing data? Organizing data is crucial for reproducibility and a good workflow in general. Below us we find an example of a folder containing multiple projects i made. This dataset is orginized by the Guerrilla Analytics Principles. Each project contains a folder “data” and “output”. And each folder contains a “README” file about the folder and data. Each “data” folder contains a “rawdata” folder. In this folder is the file with the rawdata. This data will not be used during the experiment, instead we have a copy of the dataset in “data” to prevent errors and false data from happening. ## C:/Users/Lamya/OneDrive/Documenten/dsfb2_workflows_portfolio/portofolio_bookdown/data/Guerrila_structure ## ├── metagenomics ## │ ├── metagenomics_formatief ## │ │ ├── data ## │ │ │ ├── HU2_MOCK2_L001_R1_001.fastq.gz ## │ │ │ ├── HU2_MOCK2_L001_R2_001.fastq.gz ## │ │ │ ├── HU_waternet_MOCK2_composition_copy.csv ## │ │ │ ├── rawdata ## │ │ │ │ └── HU_waternet_MOCK2_composition.csv ## │ │ │ └── README.txt ## │ │ ├── output ## │ │ │ ├── metagenomics_formatief.html ## │ │ │ └── metagenomics_formatief.Rmd ## │ │ └── README.txt ## │ ├── metagenomics_reader ## │ │ ├── data ## │ │ │ ├── HU1_MOCK1_L001_R1_001.fastq.gz ## │ │ │ ├── HU1_MOCK1_L001_R2_001.fastq.gz ## │ │ │ ├── HU_waternet_MOCK1_composition_copy.csv ## │ │ │ ├── rawdata ## │ │ │ │ ├── HU_waternet_MOCK1_composition.csv ## │ │ │ │ └── README.txt ## │ │ │ └── README.txt ## │ │ ├── output ## │ │ │ ├── metagenomics_reader.html ## │ │ │ └── metagenomics_reader.Rmd ## │ │ └── README.txt ## │ └── README.txt ## └── rna_sequencing ## ├── README.txt ## ├── rnaseq_airway ## │ ├── data ## │ │ ├── airway_sampledata_copy.csv ## │ │ ├── bam ## │ │ │ ├── READMe.txt ## │ │ │ ├── SRR1039508.bam ## │ │ │ ├── SRR1039508.bam.indel.vcf ## │ │ │ └── SRR1039508.bam.summary ## │ │ ├── counts ## │ │ │ ├── README.txt ## │ │ │ └── read_counts.rds ## │ │ ├── fastq ## │ │ │ ├── SRR1039516_1.fastq.gz ## │ │ │ ├── SRR1039516_2.fastq.gz ## │ │ │ ├── SRR1039517_1.fastq.gz ## │ │ │ └── SRR1039520_1.fastq.gz ## │ │ ├── rawdata ## │ │ │ └── airway_sampledata.csv ## │ │ └── README.txt ## │ ├── output ## │ │ └── fastqc_output ## │ │ ├── SRR1039508_1_fastqc.html ## │ │ ├── SRR1039508_1_fastqc.zip ## │ │ ├── SRR1039508_2_fastqc.html ## │ │ └── SRR1039508_2_fastqc.zip ## │ └── README.txt ## ├── rnaseq_ipsc ## │ ├── data ## │ │ ├── bam ## │ │ │ ├── SRR7866687.bam ## │ │ │ ├── SRR7866687.bam.indel.vcf ## │ │ │ └── SRR7866687.bam.summary ## │ │ ├── counts ## │ │ │ └── read_counts.rds ## │ │ ├── fastq ## │ │ │ ├── SRR7866693_1.fastq.gz ## │ │ │ ├── SRR7866693_2.fastq.gz ## │ │ │ ├── SRR7866694_1.fastq.gz ## │ │ │ └── SRR7866694_2.fastq.gz ## │ │ ├── ipsc_sampledata_copy.csv ## │ │ ├── rawdata ## │ │ │ └── ipsc_sampledata.csv ## │ │ └── README.txt ## │ ├── output ## │ │ ├── fastqc_output ## │ │ │ ├── README.txt ## │ │ │ ├── SRR7866687_1_fastqc.html ## │ │ │ ├── SRR7866687_1_fastqc.zip ## │ │ │ ├── SRR7866687_2_fastqc.html ## │ │ │ └── SRR7866687_2_fastqc.zip ## │ │ └── README.txt ## │ └── README.txt ## └── rnaseq_onecut ## ├── data ## │ ├── bam ## │ │ ├── SRR7866699.bam ## │ │ ├── SRR7866699.bam.indel.vcf ## │ │ └── SRR7866699.bam.summary ## │ ├── counts ## │ │ └── read_counts_OC3.rds ## │ ├── fastqc ## │ │ ├── SRR7866703_1.fastq.gz ## │ │ ├── SRR7866703_2.fastq.gz ## │ │ ├── SRR7866704_1.fastq.gz ## │ │ ├── SRR7866704_2.fastq.gz ## │ │ ├── SRR7866705_1.fastq.gz ## │ │ └── SRR7866706_2.fastq.gz ## │ ├── onecut_sampledata_OC3_copy.csv ## │ ├── rawdata ## │ │ └── onecut_sampledata_OC3.csv ## │ └── README.txt ## ├── output ## └── README.txt "],["western-blot-analysis-using-rstudio.html", "5 Western Blot analysis using Rstudio 5.1 Introduction 5.2 planning 5.3 Gel analysis and R 5.4 Getting started 5.5 Obtaining data 5.6 Working with the dataframe and plotting 5.7 A better analyse 5.8 Conclusion", " 5 Western Blot analysis using Rstudio 5.1 Introduction Western Blotting (WB) is a commonly used method in the biological scienes. This technique is used to investigate many features of the protein, ranging from basic protein analysis to disease detection such as cancer and HIV(Begum, Murugesan, and Tangutur 2022). Western Blotting involves three components to achieve this goal: (1) separating based on size, (2) transferring onto a solid surface, and (3) labeling the protein of interest with specific primary and secondary antibodies for visualization(Mahmood and Yang 2012). This results in a gel full of different proteins separated by size. Image J is often used to analyze the Western Blot results. The technique is highly sensitive and can be used to detect even small amounts of protein. It is often used in research into diseases such as cancer and HIV, where specific proteins can be used as markers for diagnosis and treatment. The question now is: Can we analise Western Blot gels in R? 5.2 planning The first thing I need to do is to read about Western blot analysis in Rstudio. We need to look if people made packages that will help us analyzing the gels and if they succeed. Most of the Western Blot analysis is done with Image J, which makes my search for R codes a bit more difficult. Find articles about photo analysis and Western blot analysis in Rstudio. Look for a way to obtain data about the pixels of photos . Try to run the code in the previous step on western blot gels. Visualize the data in a way the lanes are can be distinguished 5.3 Gel analysis and R Not many people have used R to analysis their Western Blot gels. Most of the gel analysis are done with Image J or Python. The reason for that is because R is mostly used for statistical analytics, while Python has multiple purposes. For example data analysis or machine learning. Even though it was (very) hard to find articles that contain both subjects, I found some packages and that might help me reach my goal. Note that these packages are not related to Western Blot analysis, but are made to analyse photo’s. The packages that I found that could help me are “EBImage” and “imager” 5.3.1 Installation To install the “EBImage” package we did the following: # installation &quot;EBImage&quot; if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;EBImage&quot;) To install the “imager” package, we did the following: # installation &quot;imager&quot; devtools::install_github(&quot;asgr/imager&quot;) 5.4 Getting started To analize a Western Blot gel, we first need a photo. Because we’re testing if we can analize Western Blot gels with R, we’re using a photo with a high contrast and without any smear(s). The photo we’re using is from an article “DNA Damage-Induced Phosphorylation of p53 ALleviates Inhibition by MDM2” library(EBImage) # load in &quot;EBImage&quot; package Image &lt;- readImage(&#39;data/Westernblot_photo.png&#39;) #read in westernblot phot display(Image) #display image We could manipulated the contrast to our liking. The contrast can be changed using the multiplication operator(*). if the number after the multiplication operator is 1 &gt; X, you’ll get a photo with a lower contrast. If the number after the multiplaction operator is 1 &lt; X, you’ll get a phtot with a higher contrast. Note that if you’re using multiple photo’s for an analyse, you’ll need the set the same setting for every photo. library(EBImage) #load in &quot;EBImage&quot; package Image3 &lt;- Image * 0.5 #set contrast to 0.5 Image4 &lt;- Image * 1.1 #set contrast to 1.1 display(Image3); display(Image4) #display both images next to each other. The EBImage package has more functions we could use, like cropping, Gamma Correction and Color Management. Ebimage is a good and easy to use package to give your Western Blot the right colours and contrast. Unfortenly, we can’t combine EBImage with other packages. 5.5 Obtaining data From now on, it gets a bit harder. A big difference between R and Image J when it comes to Western Blot photo’s, is that with Image J you could see which lane you’re working with and manually select the lane you wanted to analize. If a lane wasn’t imported for the analize, you could easly skip that lane. With R, we need to use the x-axis and y-axis of a photo while analyzing. 5.5.1 plotting the image and learning more about imager To plot the image, we’re using the plot function of the imager package. library(imager) im &lt;- load.image(&#39;data/Westernblot_photo.png&#39;) plot(im) The y-axis runs downwards, and the origin is located at the top-left corner, which is the conventional coordinate system for images. Imager consistently employs this coordinate system. the image data is classified as ‘cimg’. class(im) #shows class of the photo ## [1] &quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot; im #will give information about the photo ## Image. Width: 1091 pix Height: 502 pix Depth: 1 Colour channels: 4 With imager, the photo is stored as a 4D numeric array. Meaning the usual arithmetic operations works (for most of the time): log(im)+3*sqrt(im) ## Image. Width: 1091 pix Height: 502 pix Depth: 1 Colour channels: 4 mean(im) ## [1] 0.881701 sd(im) ## [1] 0.1327154 The four dimensions are labelled x,y,z,c. We’ve already seen x and y in the plot. ‘z’ stands for the depth (and if we’re using videos also time) and ‘c’ stands for the colour. Imager will always use the x,y,z,c order. A photo contains thousand of lines horzintal and vertical. Imager will scan the image beginning at the upper left corner (y=0), along the x-axis. When it’s done, it will move to the next line (y=1). It will repeat this proces until it’s done. dim(im) #shows dimentions of the photo ## [1] 1091 502 1 4 We can see that our Western Blot photo has 1091 lines on the x-axis and 502 lines on the y-axis. Because we’re using a photo, z always be 1. If we used a video, z will increase. And lastly our photo has 4 colour chanels. We could also get these numbers individually. width(im) #shows the width (x-axis) ## [1] 1091 height(im) #shows the height (y-axis) ## [1] 502 depth(im) #shows depth (z) ## [1] 1 spectrum(im) #shows colour chanels (c) ## [1] 4 Because the images that are load in by imager are arrays. We can make a dataframe of it. df &lt;- im %&gt;% as.data.frame #convert &#39;im&#39; to a data frame df %&gt;% head(10) #look at the first 10 rows of the data frame ## x y cc value ## 1 1 1 1 0.9529412 ## 2 2 1 1 0.9568627 ## 3 3 1 1 0.9607843 ## 4 4 1 1 0.9568627 ## 5 5 1 1 0.9568627 ## 6 6 1 1 0.9647059 ## 7 7 1 1 0.9647059 ## 8 8 1 1 0.9607843 ## 9 9 1 1 0.9607843 ## 10 10 1 1 0.9647059 The data frame contains 3 colomns. The first 2 give us the x-y coordinates. The third colomn gives us a value. The ‘value’ is the intensity of the pixel on that place, how lighter the pixel, the higher the value. There are a few problems, this dataframe has 547,682 rows, which could be a lot to plot or even work with. And a bigger problem is that we don’t exactly know which values belong to which lanes and which straps. Luckily imager has something that might help. We want to locate our straps in the gel. First we’ll get the pixels with the highest values, we threshold the image. threshold(im,&quot;10%&quot;) %&gt;% plot(main=&quot;Determinant: 90% highest values&quot;) #In this cases we used 10%, test for your own pictures what is best As you can see the straps are now darker, and the background is lighter, but imager gives higher values to lighter pixels. We’ll change that using the 255 - 'image' formula. lab &lt;- threshold(im,&quot;10%&quot;) #filter the image lab_r &lt;- 255 - lab #making the image reversed plot(lab_r,main=&quot;Labelled straps&quot;) #plot the image There is still a lot of noise going around. We want to clear the photo a bit. To do this we use isoblur library(imager) #load in &#39;imager&#39; package nlab.denoised &lt;- isoblur(lab_r,4) #blur the image with &#39;4&#39; plot(nlab.denoised) #plot the image clearing up the noise is not that good of an idea. The straps will get blury and there is still some noise going on. 5.6 Working with the dataframe and plotting While I was searching for articles, I saw a photo of a SDS-page gel with graphs on the x-axis and graphs on the y-axis(unfortnely I don’t have the source anymore). I tried different ways to recreate that image, but with my own data. After some time, I might found a solution to work with this big dataframe. lab_df &lt;- lab_r %&gt;% as.data.frame() #make a dataframe of the image The first step is to group the x values and the y values, we do this with the group_by function. library(dplyr) #load in dplyr package data_of_x &lt;- lab_df %&gt;% group_by(x) %&gt;% summarize(mean_data = mean(value)) #group by &#39;x&#39; and calculate the mean value data_of_y &lt;- lab_df %&gt;% group_by(y) %&gt;% summarize(mean_data = mean(value)) #group by &#39;y&#39; and calculate the mean value When we have everything grouped, we can plot our data. library(ggplot2) #load in &#39;ggplot2&#39; #plot the data of x data_of_x %&gt;% ggplot(aes(y = mean_data, x = x)) + geom_line() + labs(title = &quot;Mean value of the pixels on the x-axis&quot;, x = &quot;x-axis&quot;, y = &quot;Mean value of the pixels&quot;) #plot the data of y data_of_y %&gt;% ggplot(aes(y = mean_data, x = y)) + geom_line() + labs(title = &quot;Mean value of the pixels on the y-axis&quot;, x = &quot;y-axis&quot;, y = &quot;Mean value of the pixels&quot;) We finally got a plot that shows us the pixel values. To make the y-graph more understandable, we can flip the x- and y-axis with coord_flip data_of_y %&gt;% ggplot(aes(y = mean_data, x = y)) + geom_line() + coord_flip() + #flip the x- and y-axis scale_x_reverse() + #reverse the x-axis values labs(title = &quot;Mean value of the pixels on the y-axis&quot;, x = &quot;y-axis&quot;, y = &quot;Mean value of the pixels&quot;) First we’ll look at the graph of the x-axis. As we can see, the plots are kinda rough. This is because of all the noise. There are more things that are wrong in this plot, there are more straps in one lane. We can’t tell the difference between the two straps in one lane. We can’t compare the data to each other because of this. If we want to compare the samples, we have to crop the image in different rows. Using the x-axis is great when you have a different lanes with just one strap. Secondly we’ll look at the graph of the y-axis. The same problem occurs here. There are multiple straps under one peak, because of this we cant tell the difference between strap 1 and strap 2 in one lane. What we can conclude is that there are two rows, and that the first row has a higher intensity than the second row. We’ve also reversed the y-axis, because as we saw in the beginning this is how imager plots their images. The last thing we can notice is the noise, especially around y = 230 5.7 A better analyse The previous plots couldn’t be trusted. There were multiple straps under one peak. We could help this by cropping the photo. We wanna make a graph about the x-axis and a graph about the y-axis. For the x-axis graph, we’re gonna crop the image in two pieces horizontally. We’ll do this manually, since imager can’t put these two pieces in different objects. For the x-axis graph, we now have two images. One image containing the first row, and the second image containing the second row. library(imager) #load in &#39;imager&#39; package wbx_r1 &lt;- load.image(&#39;data/wbx_row1.png&#39;) #loading &quot;Western Blot X-axis row 1&quot; image wbx_r2 &lt;- load.image(&#39;data/wbx_row2.png&#39;) #loading &quot;Western Blot X-axis row 2&quot; image #plot images plot(wbx_r1) plot(wbx_r2) For the y-axis graph, we’re gonna do something familiar. We’ll cut the image vertically. This image has 6 lanes, but for this final product we’re only gonna use two lanes. We’ll use the 4th and the 5th lane, since these two have one of the biggest difference. library(imager) wby_l4 &lt;- load.image(&#39;data/wby_lane4.png&#39;) #loading &quot;Western Blot Y-axis lane 4&quot; image wby_l5 &lt;- load.image(&#39;data/wby_lane5.png&#39;) #loading &quot;Western Blot Y-axis lane 5&quot; image #plot images plot(wby_l4) plot(wby_l5) We now have four images we can work with. 5.7.1 obtaining the data Before we go any further, we need to delete colour from our images #delete color from our images wbx_r1b &lt;- imsplit(wbx_r1,&quot;c&quot;) %&gt;% add wbx_r2b &lt;- imsplit(wbx_r2,&quot;c&quot;) %&gt;% add wby_l4b &lt;- imsplit(wby_l4,&quot;c&quot;) %&gt;% add wby_l5b &lt;- imsplit(wby_l5,&quot;c&quot;) %&gt;% add spectrum(wbx_r1b) #proof we only have 1 colour chanel now ## [1] 1 we’ll now make the difference between the background and the straps higher. r1_lab &lt;- threshold(wbx_r1b,&quot;10%&quot;) #Make contrast higher plot(r1_lab,main=&quot;Straps row 1&quot;) #plot the image r2_lab &lt;- threshold(wbx_r2b ,&quot;10%&quot;) plot(r2_lab,main=&quot;Straps row 2&quot;) l4_lab &lt;- threshold(wby_l4b,&quot;10%&quot;) plot(l4_lab,main=&quot;Straps lane 4&quot;) l5_lab &lt;- threshold(wby_l5b,&quot;10%&quot;) plot(l5_lab,main=&quot;Straps lane 5&quot;) Because imager gives the lighter pixels (white) a higher value than the darker pixels (black), we need to reverse the images. We can do this with the 255 - 'ímage' formula. #reverse images r1_lab_r &lt;- 255 - r1_lab r2_lab_r &lt;- 255 - r2_lab l4_lab_r &lt;- 255 - l4_lab l5_lab_r &lt;- 255 - l5_lab plot(r1_lab_r ,main=&quot;Straps row 1 reversed&quot;) We’ve now prepared our images and can finally obtain the dataframes we’re gonna use the make the plot. #make dataframe of reversed images r1_lab_df &lt;- r1_lab_r %&gt;% as.data.frame() r2_lab_df &lt;- r2_lab_r %&gt;% as.data.frame() l4_lab_df &lt;- l4_lab_r %&gt;% as.data.frame() l5_lab_df &lt;- l5_lab_r %&gt;% as.data.frame() r1_lab_df %&gt;% head(10) #show the first 10 rows of a dataframe ## x y value ## 1 1 1 254 ## 2 2 1 254 ## 3 3 1 254 ## 4 4 1 254 ## 5 5 1 254 ## 6 6 1 254 ## 7 7 1 254 ## 8 8 1 254 ## 9 9 1 254 ## 10 10 1 254 The last step before plotting is to group the X-axis and Y-axis values using group_by, and calculate the mean. library(dplyr) #load in dplyr package #group by &#39;x&#39; and calculate the mean value data_of_r1 &lt;- r1_lab_df %&gt;% group_by(x) %&gt;% summarize(mean_data = mean(value)) data_of_r2 &lt;- r2_lab_df %&gt;% group_by(x) %&gt;% summarize(mean_data = mean(value)) #group by &#39;y&#39; and calculate the mean value data_of_l4 &lt;- l4_lab_df %&gt;% group_by(y) %&gt;% summarize(mean_data = mean(value)) data_of_l5 &lt;- l5_lab_df %&gt;% group_by(y) %&gt;% summarize(mean_data = mean(value)) data_of_r2 %&gt;% head(10) #show the first 10 rows of the grouped data ## # A tibble: 10 × 2 ## x mean_data ## &lt;int&gt; &lt;dbl&gt; ## 1 1 254. ## 2 2 254. ## 3 3 254. ## 4 4 254. ## 5 5 254. ## 6 6 254. ## 7 7 254. ## 8 8 254. ## 9 9 254. ## 10 10 254. Finally, we can plot our data! 5.7.2 Step 3: Plotting the data First we’ll plot the x-axis graph. library(ggplot2) #load in &#39;ggplot2&#39; # plot dataframes data_of_r1 %&gt;% ggplot(aes(y = mean_data, x = x)) + geom_line() + labs(title = &quot;Mean value of the pixels on the x-axis on row 1&quot;, x = &quot;x-axis&quot;, y = &quot;Mean value of the pixels on row 1&quot;) #plot dataframes data_of_r2 %&gt;% ggplot(aes(y = mean_data, x = x)) + geom_line() + labs(title = &quot;Mean value of the pixels on the x-axis on row 2&quot;, x = &quot;x-axis&quot;, y = &quot;Mean value of the pixels on row 2&quot;) We’ll look at the plot of row 1 first. The first 3 lanes aren’t as visible on the photo as the last 3. And thats exactly what we see in the plot. If we compare the last 3 with one and another, we can see peak 4 (x = 630) is a lot smaller than the other 3. With the last one being the biggest. The second plot looks a bit different than the first one. In this plot, the last peak is the smallest. In the first 3 peaks, we see a lot of noise going around, and that’s exactly what happened on the image. After reviewing the plots of the x-axis, we’ll now look at the plots of the y-axis. library(ggplot2) #load in &#39;ggplot2&#39; # plot dataframes data_of_l4 %&gt;% ggplot(aes(y = mean_data, x = y)) + geom_line() + coord_flip() + scale_x_reverse() + labs(title = &quot;Mean value of the pixels on the y-axis on row 1&quot;, x = &quot;y-axis&quot;, y = &quot;Mean value of the pixels on row 1&quot;) #plot dataframe data_of_l5 %&gt;% ggplot(aes(y = mean_data, x = y)) + geom_line() + coord_flip() + scale_x_reverse() + labs(title = &quot;Mean value of the pixels on the y-axis on row 2&quot;, x = &quot;y-axis&quot;, y = &quot;Mean value of the pixels on row 2&quot;) In both plots, the peaks don’t really have that big of a difference. If we compare the first plot to the second one we see the same problem as with the second x-axis plot, the noise. luckily the first plot doesn’t get that much affected by the noise. What we can conlcude in these plots is that the first peak is slightly thicker than the second one. 5.8 Conclusion To get back at the quistion we asked at the beginning, yes, we can indeed analise Western Blot photo’s with R. Analysing the images wasn’t an easy job to do. Making this analise costs a lot of trial and error. But once I firgured what i needed to do, it was queit easy. Analysing with R is great if you wanna compare gels that look like the image we used. But if it gets complicater, think about smears, different colour, it get’s harder to make a good, readable graph. Because as we’ve seen, a dataframe of an image becomes a very big dataframe. References "],["relational-databases.html", "6 Relational databases 6.1 Why using relational databases? 6.2 The database", " 6 Relational databases 6.1 Why using relational databases? Relational databases are useful for organizing and managing large amounts of data in an organized way, allowing users to easy find and manipulate data. They also make sure that the data is accurate and consistent. Meaning that the data in the databse is the same across all records and tables. it’s important because it ensures that the data can be trusted and used for imported tasks. 6.2 The database First we gonna load in the datasets into three separated dataframes in R. #load in required packages library(tidyverse) library(dslabs) #load in gapminder dataset as dataframe gapminder_data &lt;- as.data.frame(gapminder) head(gapminder_data) #show the first 12 rows of the dataframe ## country year infant_mortality life_expectancy fertility ## 1 Albania 1960 115.40 62.87 6.19 ## 2 Algeria 1960 148.20 47.50 7.65 ## 3 Angola 1960 208.00 35.98 7.32 ## 4 Antigua and Barbuda 1960 NA 62.97 4.43 ## 5 Argentina 1960 59.87 65.39 3.11 ## 6 Armenia 1960 NA 66.86 4.55 ## population gdp continent region ## 1 1636054 NA Europe Southern Europe ## 2 11124892 13828152297 Africa Northern Africa ## 3 5270844 NA Africa Middle Africa ## 4 54681 NA Americas Caribbean ## 5 20619075 108322326649 Americas South America ## 6 1867396 NA Asia Western Asia #load in dengue dataset dengue_data &lt;- read.csv(&quot;./data/raw_data/dengue_data.csv&quot;, skip = 10) dengue_df &lt;- as.data.frame(dengue_data) #make dataframe head(dengue_df) #show the first 12 rows of the dataframe ## Date Argentina Bolivia Brazil India Indonesia Mexico Philippines ## 1 2002-12-29 NA 0.101 0.073 0.062 0.101 NA NA ## 2 2003-01-05 NA 0.143 0.098 0.047 0.039 NA NA ## 3 2003-01-12 NA 0.176 0.119 0.051 0.059 0.071 NA ## 4 2003-01-19 NA 0.173 0.170 0.032 0.039 0.052 NA ## 5 2003-01-26 NA 0.146 0.138 0.040 0.112 0.048 NA ## 6 2003-02-02 NA 0.160 0.202 0.038 0.049 0.041 NA ## Singapore Thailand Venezuela ## 1 0.059 NA NA ## 2 0.059 NA NA ## 3 0.238 NA NA ## 4 0.175 NA NA ## 5 0.164 NA NA ## 6 0.163 NA NA #load in flu dataset flu_data &lt;- read.csv(&quot;./data/raw_data/flu_data.csv&quot;, skip = 10) flu_df &lt;- as.data.frame(flu_data) #make datafram of flu_data head(flu_df) #show the first 12 rows of the dataframe ## Date Argentina Australia Austria Belgium Bolivia Brazil Bulgaria Canada ## 1 2002-12-29 NA NA NA NA NA 174 NA NA ## 2 2003-01-05 NA NA NA NA NA 162 NA NA ## 3 2003-01-12 NA NA NA NA NA 174 NA NA ## 4 2003-01-19 NA NA NA NA NA 162 NA NA ## 5 2003-01-26 NA NA NA NA NA 131 NA NA ## 6 2003-02-02 136 NA NA NA NA 151 NA NA ## Chile France Germany Hungary Japan Mexico Netherlands New.Zealand Norway ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 1 NA NA NA NA NA NA NA NA ## 4 0 NA NA NA NA NA NA NA NA ## 5 0 NA NA NA NA NA NA NA NA ## 6 0 NA NA NA NA NA NA NA NA ## Paraguay Peru Poland Romania Russia South.Africa Spain Sweden Switzerland ## 1 NA 329 NA NA NA NA NA NA NA ## 2 NA 315 NA NA NA NA NA NA NA ## 3 NA 314 NA NA NA NA NA NA NA ## 4 NA 267 NA NA NA NA NA NA NA ## 5 NA 241 NA NA NA NA NA NA NA ## 6 NA 227 NA NA NA NA NA NA NA ## Ukraine United.States Uruguay ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA In the tibbles above us we see the gapminder, flu and the dengue dataframes. The gapminder dataframe is tidy, there is only one observation in each row The flu dataframe and the dengue dataframe are not tidy as we can see in the dataframes above us. There are more than one observation in each row. To make both dataframes tidy we use {privot_longer}. #make dengue dataframe tidy using &#39;privot_longer&#39; dengue_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;dengue_cases&quot;) #show the first 12 rows of the dataset head(dengue_tidy) ## # A tibble: 6 × 3 ## Date country dengue_cases ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Bolivia 0.101 ## 3 2002-12-29 Brazil 0.073 ## 4 2002-12-29 India 0.062 ## 5 2002-12-29 Indonesia 0.101 ## 6 2002-12-29 Mexico NA #make flu dataframe tidy using &#39;privot_longer&#39; flu_tidy&lt;- flu_df %&gt;% pivot_longer(cols = Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;flu_cases&quot;) #show the first 12 rows of the dataset head(flu_tidy) ## # A tibble: 6 × 3 ## Date country flu_cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Australia NA ## 3 2002-12-29 Austria NA ## 4 2002-12-29 Belgium NA ## 5 2002-12-29 Bolivia NA ## 6 2002-12-29 Brazil 174 #show the first 12 rows of the dataset head(gapminder) ## country year infant_mortality life_expectancy fertility ## 1 Albania 1960 115.40 62.87 6.19 ## 2 Algeria 1960 148.20 47.50 7.65 ## 3 Angola 1960 208.00 35.98 7.32 ## 4 Antigua and Barbuda 1960 NA 62.97 4.43 ## 5 Argentina 1960 59.87 65.39 3.11 ## 6 Armenia 1960 NA 66.86 4.55 ## population gdp continent region ## 1 1636054 NA Europe Southern Europe ## 2 11124892 13828152297 Africa Northern Africa ## 3 5270844 NA Africa Middle Africa ## 4 54681 NA Americas Caribbean ## 5 20619075 108322326649 Americas South America ## 6 1867396 NA Asia Western Asia All three datasets are now tidy. Each row contains their own observation. If we compare the colomn type of the flu and dengue dataset, we can see some difference in these datasets. The colomns ‘year’ and ‘country’ don’t have the same variables and data type as the gapminder dataset. We need to change these colomn so they match with each other. library(tidyverse) dengue_tidy$country &lt;- as.factor(dengue_tidy$country) #transform data type to factor dengue_tidy &lt;- dengue_tidy %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), convert = T, sep = &quot;-&quot;) #separate &#39;date&#39; into &#39;year&#39;, &#39;month&#39; and &#39;day&#39; flu_tidy$country &lt;- as.factor(flu_tidy$country) #transform data type to factor flu_tidy &lt;- flu_tidy %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), convert = T, sep = &quot;-&quot;) #separate &#39;date&#39; into &#39;year&#39;, &#39;month&#39; and &#39;day&#39; dengue_tidy %&gt;% head(5) ## # A tibble: 5 × 5 ## year month day country dengue_cases ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Bolivia 0.101 ## 3 2002 12 29 Brazil 0.073 ## 4 2002 12 29 India 0.062 ## 5 2002 12 29 Indonesia 0.101 flu_tidy %&gt;% head(5) ## # A tibble: 5 × 5 ## year month day country flu_cases ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Australia NA ## 3 2002 12 29 Austria NA ## 4 2002 12 29 Belgium NA ## 5 2002 12 29 Bolivia NA We’re now gonna store each tables we created as .csv and .rds files. #store dataset as .csv file dengue_tidy %&gt;% write.csv(&quot;data/denguetidy.csv&quot;) flu_tidy %&gt;% write.csv(&quot;data/flutidy.csv&quot;) gapminder %&gt;% write.csv(&quot;data/gapminder.csv&quot;) #store dataset as .rds file dengue_tidy %&gt;% write_rds(&quot;data/denguetidy.rds&quot;) flu_tidy %&gt;% write_rds(&quot;data/flutidy.rds&quot;) gapminder %&gt;% write_rds(&quot;data/gapminder.rds&quot;) For the next part of this analyse, we need a few packages to use Dbeaver and PostgreSQL. We downloaded the following packages. install.packages(&#39;RPostgreSQL&#39;) install.packages(&#39;devtools&#39;) install.packages(&#39;remotes&#39;) install.packages(&#39;RPostgres&#39;) We have created a new databse called ‘workflowsdb’ in Dbeaver. To do this right click on postgres &gt; SQL Editor &gt; Open SQL console. In the console we created the new database with the following code: CREATE DATABASE workflowsdb To insert these three datasets, we need to connect to the workflows database we just created. Once we are connected, we’re gonna insert the tables into the database. #load in DBI package library(DBI) #connect to Dbeaver workflows database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;password&quot;) #password can be renewed with &quot;ALTER USER user_name WITH PASSWORD &#39;new_password&#39;;&quot; #Make tables dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) dbWriteTable(con, &quot;flu&quot;, flu_tidy) dbWriteTable(con, &quot;gapminder&quot;, gapminder_data) We need to know if everything is imported correctly. We need to inspect the contents of the tables with SQL and in R. #load in DBI package library(DBI) #inspect if datasets are corrrectly imported dengue_s &lt;- dbReadTable(con, &quot;dengue&quot;) flu_s &lt;- dbReadTable(con, &quot;flu&quot;) gapminder_s &lt;- dbReadTable(con, &quot;gapminder&quot;) dengue_s %&gt;% head(5) #the _s stands for SQL ## year month day country dengue_cases ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Bolivia 0.101 ## 3 2002 12 29 Brazil 0.073 ## 4 2002 12 29 India 0.062 ## 5 2002 12 29 Indonesia 0.101 flu_s %&gt;% head(5) ## year month day country flu_cases ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Australia NA ## 3 2002 12 29 Austria NA ## 4 2002 12 29 Belgium NA ## 5 2002 12 29 Bolivia NA gapminder_s %&gt;% head(5) ## country year infant_mortality life_expectancy fertility ## 1 Albania 2002 21.0 75.5 2.20 ## 2 Algeria 2002 32.4 73.8 2.41 ## 3 Angola 2002 125.5 53.3 6.78 ## 4 Antigua and Barbuda 2002 12.3 74.3 2.29 ## 5 Argentina 2002 17.1 74.3 2.38 ## population gdp continent region ## 1 3123112 4059111575 Europe Southern Europe ## 2 31990387 58856686557 Africa Northern Africa ## 3 16109696 10780448534 Africa Middle Africa ## 4 80030 840972692 Americas Caribbean ## 5 37889443 242076212334 Americas South America #make a summary of the imported data form SQL summary(dengue_s) ## year month day country ## Min. :2002 Min. : 1.000 Min. : 1.0 Length:6590 ## 1st Qu.:2006 1st Qu.: 3.000 1st Qu.: 8.0 Class :character ## Median :2009 Median : 6.000 Median :16.0 Mode :character ## Mean :2009 Mean : 6.414 Mean :15.7 ## 3rd Qu.:2012 3rd Qu.: 9.000 3rd Qu.:23.0 ## Max. :2015 Max. :12.000 Max. :31.0 ## ## dengue_cases ## Min. :0.0000 ## 1st Qu.:0.0530 ## Median :0.1040 ## Mean :0.1390 ## 3rd Qu.:0.1755 ## Max. :1.0000 ## NA&#39;s :327 summary(flu_s) ## year month day country ## Min. :2002 Min. : 1.000 Min. : 1.0 Length:19111 ## 1st Qu.:2006 1st Qu.: 3.000 1st Qu.: 8.0 Class :character ## Median :2009 Median : 6.000 Median :16.0 Mode :character ## Mean :2009 Mean : 6.414 Mean :15.7 ## 3rd Qu.:2012 3rd Qu.: 9.000 3rd Qu.:23.0 ## Max. :2015 Max. :12.000 Max. :31.0 ## ## flu_cases ## Min. : 0.0 ## 1st Qu.: 37.0 ## Median : 185.0 ## Mean : 473.7 ## 3rd Qu.: 578.0 ## Max. :10555.0 ## NA&#39;s :1845 summary(gapminder_s) ## country year infant_mortality life_expectancy ## Length:2590 Min. :2002 Min. : 1.50 Min. :32.20 ## Class :character 1st Qu.:2005 1st Qu.: 7.70 1st Qu.:64.60 ## Mode :character Median :2008 Median : 19.05 Median :72.60 ## Mean :2008 Mean : 29.59 Mean :70.35 ## 3rd Qu.:2012 3rd Qu.: 47.00 3rd Qu.:77.00 ## Max. :2015 Max. :137.70 Max. :83.73 ## NA&#39;s :98 ## fertility population gdp continent ## Min. :0.840 Min. :5.619e+04 Min. :7.024e+07 Length:2590 ## 1st Qu.:1.780 1st Qu.:2.037e+06 1st Qu.:3.338e+09 Class :character ## Median :2.410 Median :7.221e+06 Median :1.278e+10 Mode :character ## Mean :2.929 Mean :3.600e+07 Mean :2.103e+11 ## 3rd Qu.:3.850 3rd Qu.:2.292e+07 3rd Qu.:8.483e+10 ## Max. :7.680 Max. :1.376e+09 Max. :1.174e+13 ## NA&#39;s :2 NA&#39;s :792 ## region ## Length:2590 ## Class :character ## Mode :character ## ## ## ## We now want to join these 3 databases. To join these three datasets together we need to clean the gapminder data so it could join the dengue and flue dataset. We saw that all 3 datasets have a colomn called ‘year’ and ‘country’. We already changed the colomn ‘year’ and ‘country’ in the flu en dengue dataset to match the gapminder dataset. First we need to check their ranges in the colomn ‘year’. What’s their minimal year and what is their max year and how many years does each dataset contain? library(tidyverse) #check the ranges of each dataset and put them in a list gapminder_data$year %&gt;% unique() ## [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 ## [16] 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 ## [31] 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 ## [46] 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 dengue_tidy$year %&gt;% unique() ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 flu_tidy$year %&gt;% unique() ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 As we can see, the gapminder has a wider range than the flu and dengue datasets. We’ll remove the years that aren’t in the flu and dengue dataset so we could join these three datasets together. #remove the years that the flu and dengue dataset not contain gapminder_data &lt;- gapminder %&gt;% subset(year %in% c(2002:2015) ) gapminder_data$year %&gt;% unique() ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 The gapminder dataset has now the same range as the flu and dengue dataset. we’ll remove the old gapminder dataset for the new clean dataset named “gapminder_data” in Dbeaver with SQL. #replace the old gapminder dataset with the new gapminder dataset dbRemoveTable(con, &quot;gapminder&quot;) dbWriteTable(con, &quot;gapminder&quot;, gapminder_data) #import the new &quot;clean&quot; dataset from SQL gapminder_s &lt;- dbReadTable(con, &quot;gapminder&quot;) gapminder_s$year %&gt;% unique() ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 We can now join all three datasets together in DBeaver with SQL. we’re using inner_join to make a dataset with variables that are available in both datasets. we’re using inner_join to make a dataset with variables that are available in both datasets. #inner join the flu and dengue dataset flu_dengue &lt;- inner_join( flu_tidy, dengue_tidy, by = c(&quot;country&quot;, &quot;year&quot;) ) #inner join the gapminder dataset with flu_dengue gapminder_dengue_flu &lt;- inner_join( gapminder_data, flu_dengue, by = c(&quot;country&quot;, &quot;year&quot;) ) #left join gapminder with flu gapminder_flu &lt;- left_join( gapminder_data, flu_tidy, by = c(&quot;country&quot;, &quot;year&quot;) ) #left join gapminder with dengue gapminder_dengue &lt;- left_join( gapminder_data, dengue_tidy, by = c(&quot;country&quot;, &quot;year&quot;) ) Now we’ve joined the three different datasets in four different ways. We’ve inner joined the flu with the dengue dataset, and the gapminder with the flu and dengue dataset. We’ve also left joined the gapminder with the flu dataset and the gapminder with the dengue dataset. The meanings of these joins can be found here. gapminder_dengue_flu %&gt;% ggplot() + geom_col(aes(x = country, y = flu_cases, fill = country) ) + theme(axis.text.x = element_text(angle = 25), legend.position = &quot;none&quot; ) + labs(title = &quot;Flu cases per country&quot;, subtitle = &quot;flu cases per country around the world in total&quot;, x = &quot;country&quot;, y = &quot;flu cases&quot; ) As we can see, Mexico has the most flu cases gapminder_dengue_flu %&gt;% ggplot() + geom_col(aes(x = country, y = dengue_cases, fill = country) ) + theme(axis.text.x = element_text(angle = 25), legend.position = &quot;none&quot; ) + labs(title = &quot;Dengue cases per country&quot;, subtitle = &quot;dengue cases per country around the world in total&quot;, x = &quot;country&quot;, y = &quot;dengue cases&quot; ) When we look at dengue cases, Bolivia has the least. flu_dengue_2003 &lt;- flu_dengue %&gt;% filter(year==2003) correlation&lt;-cor.test(flu_dengue_2003$flu_cases, flu_dengue_2003$dengue_cases, method=c(&quot;pearson&quot;))$estimate %&gt;% round(digits = 3) ggplot(data = flu_dengue_2003, aes(x = flu_cases, y = dengue_cases)) + geom_point(aes(color = country, shape = country), size = 0.8,alpha = 0.8, width = 0.2)+ labs(title = &quot;Relation between flu cases and dengue cases&quot;, y = &quot;dengue cases&quot;, x = &quot;flu cases&quot;) + theme_minimal() We just made a scatterplot with he dengue and flu cases. Unfortunately, there is no relation between the dengue cases and the flu cases. "],["parameters-and-presentations.html", "7 Parameters and Presentations 7.1 Why using parameters? 7.2 The report", " 7 Parameters and Presentations 7.1 Why using parameters? Parameters make reports dynamic and customizable. They save time and effort by allowing you to create a single report that can display different results or data sets. Parameters also make reports more interactive and user-friendly, and that’s what we want! 7.2 The report For this report we’re gonna use a dataset about the daily number of new reported COVID-19 cases and deaths by EU/EEA country. We first need the import the data we’re gonna use for this report. library(tidyverse) EDCC_data &lt;- read.csv(&quot;data/edcc_data.csv&quot;) #importing dataset head(EDCC_data) #checking if data is imported correctly ## dateRep day month year cases deaths countriesAndTerritories geoId ## 1 23/10/2022 23 10 2022 3557 0 Austria AT ## 2 22/10/2022 22 10 2022 5494 4 Austria AT ## 3 21/10/2022 21 10 2022 7776 4 Austria AT ## 4 20/10/2022 20 10 2022 8221 6 Austria AT ## 5 19/10/2022 19 10 2022 10007 8 Austria AT ## 6 18/10/2022 18 10 2022 13204 7 Austria AT ## countryterritoryCode popData2020 continentExp ## 1 AUT 8901064 Europe ## 2 AUT 8901064 Europe ## 3 AUT 8901064 Europe ## 4 AUT 8901064 Europe ## 5 AUT 8901064 Europe ## 6 AUT 8901064 Europe Looking at the data, we can see that this dataset is already tidy, except the colomn “dataRep”, but we’re not gonna use that colomn. That saves us some work (thankfully). For this report, we’re gonna study the COVID-19 cases and the COVID-19 related deaths in The Netherlands in March 2021. We choose September (month=11) because it’s the first month after the big summer break. Even though almost everything was online, Almost everyone is now heading back to school. Will the COVID-19 cases and related deaths increase during this month? We set the params in the YAML header as following. --- params: country: &quot;Netherlands&quot; year: 2021 month: 9 --- We’re gonna plot the data. We’re gonna make two graphs, one for the COVID-19 cases and one for the COVID-19 related deaths. To do this we need to filter the imported data first with our params. library(tidyverse) #load in tidyverse package data_params &lt;- EDCC_data %&gt;% filter(countriesAndTerritories == params$country, year == params$year, month == params$month) #creating a new dataset with our params head(data_params) #checking if the data is correctly filtered ## dateRep day month year cases deaths countriesAndTerritories geoId ## 1 30/09/2021 30 9 2021 1701 2 Netherlands NL ## 2 29/09/2021 29 9 2021 1751 6 Netherlands NL ## 3 28/09/2021 28 9 2021 1717 8 Netherlands NL ## 4 27/09/2021 27 9 2021 1386 4 Netherlands NL ## 5 26/09/2021 26 9 2021 1599 1 Netherlands NL ## 6 25/09/2021 25 9 2021 1600 2 Netherlands NL ## countryterritoryCode popData2020 continentExp ## 1 NLD 17407585 Europe ## 2 NLD 17407585 Europe ## 3 NLD 17407585 Europe ## 4 NLD 17407585 Europe ## 5 NLD 17407585 Europe ## 6 NLD 17407585 Europe Now that we’ve our wanted dataset, we can now plot the data. library(ggplot2) data_params %&gt;% ggplot(aes(x = day, y = cases)) + geom_line() + geom_point() + labs(title = &quot;COVID-19 cases in September in 2021 in The Netherlands&quot;, y = &quot;Number of COVID-19 cases&quot;, x = &quot;Day in September&quot;) library(ggplot2) data_params %&gt;% ggplot(aes(x = day, y = deaths)) + geom_line() + geom_point() + labs(title = &quot;COVID-19 cases in September in 2021 in The Netherlands&quot;, y = &quot;Number of COVID-19 related deaths&quot;, x = &quot;Day in September&quot;) We’ve now plotted two graphs. In the first graph we can see the COVID-19 cases in September in 2021 in The Netherlands. We can see that the cases are decreasing, but when we look at the COVID-19 related deaths we see that there’s no increasing or decreasing. "],["rpackage-lamyflow.html", "8 Rpackage: lamyflow 8.1 Why building this Rpackage? 8.2 Installation 8.3 vignette 8.4 Datasets 8.5 Functions 8.6 Github repository", " 8 Rpackage: lamyflow 8.1 Why building this Rpackage? We’re almost at the end of this portfolio, and we’ve seen a lot of datasets, codes, functions etc. To reproduce this portfolio alot easier I’ve created this Rpackage called lamyflow. 8.2 Installation The package “lamyflow” needs to be installed as following: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;lamy-m/lamyflow&quot;, build_vignettes = TRUE, force = TRUE) After installation the package can be called as following: library(lamyflow) 8.3 vignette To know more about the package, go the vignette with the following code: browseVignettes(&quot;lamyflow&quot;) 8.4 Datasets This package contains four datasets. To load in the wanted datasets use the function {data}. The following datasets are available. #show the first 10 rows of the datasets Celegans %&gt;% head(5) ## # A tibble: 5 × 34 ## plateRow plateColumn vialNr dropCode expType expReplicate expName ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NA NA 1 a experiment 3 CE.LIQ.FLOW.062 ## 2 NA NA 1 b experiment 3 CE.LIQ.FLOW.062 ## 3 NA NA 1 c experiment 3 CE.LIQ.FLOW.062 ## 4 NA NA 1 d experiment 3 CE.LIQ.FLOW.062 ## 5 NA NA 1 e experiment 3 CE.LIQ.FLOW.062 ## # ℹ 27 more variables: expDate &lt;dttm&gt;, expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;, ## # expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;dbl&gt;, compCASRN &lt;chr&gt;, ## # compName &lt;chr&gt;, compConcentration &lt;dbl&gt;, compUnit &lt;chr&gt;, ## # compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;, ## # elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, ## # bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, ## # bacterialVolUnit &lt;chr&gt;, incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, … dengue %&gt;% head(5) ## X year month day country dengue_cases ## 1 1 2002 12 29 Argentina NA ## 2 2 2002 12 29 Bolivia 0.101 ## 3 3 2002 12 29 Brazil 0.073 ## 4 4 2002 12 29 India 0.062 ## 5 5 2002 12 29 Indonesia 0.101 edcc_data %&gt;% head(5) ## dateRep day month year cases deaths countriesAndTerritories geoId ## 1 23/10/2022 23 10 2022 3557 0 Austria AT ## 2 22/10/2022 22 10 2022 5494 4 Austria AT ## 3 21/10/2022 21 10 2022 7776 4 Austria AT ## 4 20/10/2022 20 10 2022 8221 6 Austria AT ## 5 19/10/2022 19 10 2022 10007 8 Austria AT ## countryterritoryCode popData2020 continentExp ## 1 AUT 8901064 Europe ## 2 AUT 8901064 Europe ## 3 AUT 8901064 Europe ## 4 AUT 8901064 Europe ## 5 AUT 8901064 Europe flu %&gt;% head(5) ## X year month day country flu_cases ## 1 1 2002 12 29 Argentina NA ## 2 2 2002 12 29 Australia NA ## 3 3 2002 12 29 Austria NA ## 4 4 2002 12 29 Belgium NA ## 5 5 2002 12 29 Bolivia NA To know more about the datasets, use ? ?Celegans 8.5 Functions This package contains four functions. Celegans_norm(): Normalizing Celegans data csv_rds(): Saves datafile to .csv and .rds file datainfo(): Gets info about a dataset q_value(): calculates Q-value To know more about the functions, use ? ?Celegans_norm() 8.6 Github repository To find more about this package. Go to the Github repositry here "],["antimicrobial-resistance-amr.html", "9 Antimicrobial resistance (AMR) 9.1 Introduction", " 9 Antimicrobial resistance (AMR) 9.1 Introduction Antimicrobial resistance (AMR) is a growing problem worldwide. It occurs when bacteria, viruses, fungi, or parasites evolve to resist the effects of antimicrobial drugs, making infections harder to treat and increasing the risk of severe illness or death. AMR is a complex issue that has been caused by several factors, including the overuse and misuse of antibiotics, poor infection control practices, and a lack of investment in new drugs(Laxminarayan et al. 2016; Ventola 2015). Antimicrobial agents work by targeting specific bacterial properties, which are referred to as targets. Examples of antimicrobial targets include cell wall synthesis, RNA polymerase, protein synthesis, cytoplasmic membrane, and folate synthesis. However, when bacteria are repeatedly exposed to a specific antimicrobial agent, they can become resistant to it. Bacteria can adapt to their environment by producing inactivating enzymes, forming efflux pumps, evolving target structures, developing cell membrane impermeability, or creating alternative metabolic pathways(Martínez 2008). One way that bacteria can pass on their resistance to other bacteria in the environment is through conjugation, which is the direct transfer of DNA from one bacterium to another. There are several approaches to addressing the issue of AMR. One approach is the discovery and development of new antimicrobial agents. However, the development of new antimicrobial agents is a slow and expensive process. Natural products have been used for centuries to treat infections, and many of these products have been found to have potent antimicrobial activity. The use of natural products as a source of new antimicrobial agents has the potential to provide a more sustainable and cost-effective solution to the problem of AMR. In this study, wastewater samples were found to contain resistant organisms of public health concern, including ESBL-producing E. coli and carbapenem-resistant Enterobacteriaceae(Schmitt et al. 2017). Antibiotics were also detected in sewage. The impact of human exposure to resistant organisms through contact with surface waters is unknown, but further investigation is recommended. Advanced technology can reduce the amount of resistant bacteria in wastewater and fertilizers. Antimicrobial stewardship is another approach that can be used to address the issue of AMR. Antimicrobial stewardship refers to the responsible use of antimicrobial agents to preserve their effectiveness and prevent the development of AMR. The implementation of antimicrobial stewardship programs in hospitals and other healthcare settings has been shown to reduce the incidence of AMR and improve patient outcomes. Public health campaigns can help raise awareness about the importance of responsible antimicrobial use. The World Health Organization (WHO) has identified AMR as one of the top ten global public health threats facing humanity(World Health Organization 2015). The WHO has called for a global action plan to address this issue and has developed a list of priority pathogens that require urgent attention. The list includes bacteria that are resistant to multiple antimicrobial agents, such as Acinetobacter baumannii, Pseudomonas aeruginosa, and Klebsiella pneumoniae. References "],["resources.html", "10 Resources 10.1 Youtube resources 10.2 References", " 10 Resources 10.1 Youtube resources YouTube video’s that helped me build this github page. Bookdown and Github Pages: click here Using DBeaver (SQL Editor): click here Tutorial on how to build a Rpackage: click here 10.2 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
