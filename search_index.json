[["index.html", "Welcome to my portfolio 1 My portfolio 1.1 introduction 1.2 My curriculum vitae", " Welcome to my portfolio Lamyae el Mahboud 2023-05-25 1 My portfolio 1.1 introduction Welcome to my portofolio. My name is Lamyae el Mahboud and im studying at the Utrecht University of Applied Sciences. In this github page you’ll find different skills. 1.2 My curriculum vitae To get me know a little better, i’ll show you my CV i made with Rstudio. "],["c.-elegans-plate-experiment-analysis.html", "2 C. elegans plate experiment analysis 2.1 Introduction 2.2 analyzing plan", " 2 C. elegans plate experiment analysis 2.1 Introduction In this exercise we will read and analyze a data file. While analyzing the data we come across a few issues we try to fix. The data we’re gonna use in this exercise was derived from an experiment in which adult C.elegans were exposed to varying concentrations of different compounds. The variables RawData (the outcome - number of offspring counted as an integer value, after incubation time), compName (the generic name of the compound/chemical), the compConcentration (the concentration of the compound), and the expType are the most important variables in this dataset. After we reviewed the Excel File. We see in the Excel file that there is no data in the colomns ‘plateRow’ en ‘plateColmn’. Some colomns arent nesecerasy like ‘expDate’, ‘expResearcher’, ‘expTime’, ‘éxpUnit’. This kind of information is metadata and should not be in your rawdata. It makes the table cluttered and information less easy to find. Some data in colomn is the same for all coditions, like ‘compDelivery’ and ‘bacterialStrain’. This kind of information is metadata and shouldn’t be in your table. after reviewing the file we want to open the excel file in R using the {readxl} package. library(tidyverse) #load in tidyverse package library(readxl) #load in readxl package data &lt;- read_excel(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) #load in the data that we want to use for this assignment Before we go ahead an start analyzing our data. We’re gonna inspect the following colomns: “Rawdata”, “compName” and “compConcentration” head(data[c(&#39;RawData&#39;, &#39;compName&#39;, &#39;compConcentration&#39;)]) #we want to see the first few lines of the colomns RawData compName and compConcentration ## # A tibble: 6 × 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 The colomn ‘Rawdata’ is a “dbl” type, this is what we expected. The colomn ‘compName’ is a “chr” type, this is what we expected . The colomn ‘compConcentration’ is a “chr” type, this is not correct since they are only numbers in the colomn. We’re gonna make a scatterplot for the data. As you can see in the code below we put the “compConcentration” on the x-axis, the “Rawdata” counts on the y-axis. For each level in “compName” we assign a colour and for each level in “expType” we assign a different symbol. library(tidyverse) #load in the tidyverse data ggplot(data = data, aes(x = compConcentration, y = RawData )) + geom_point(aes(color = compName, shape = expType)) #scatterplot for the different compounds and the varying concentrations We just made a scatterplot with the data from the excel file we imported earlier. Each compName has its own colour and each expType has its own shape, but we see one big problem. As we can see is the x-axis not very readable, actually not at all! when we imported the Excel file, the colomn ‘compConcentration’ had a charachter type. Because of this every single charachter in each cell was shown on the x-axis and the numbers weren’t rounded. This explains why the x-axis wasn’t readable. To correct this mistake, we’re gonna change the type of the colomn “compConcentration” from “chr” to “dbl”. data$compConcentration &lt;- as.numeric(data$compConcentration) #change the type of the colomn from &lt;chr&gt; to &lt;dbl&gt; library(tidyverse) #load in tidyverse package ggplot(data = data, aes(x = log10(compConcentration), y = RawData)) + geom_jitter(aes(color = compName, shape = expType), size = 3,alpha = 0.8) + labs(title = &quot;Rawdata of each compName and their expType&quot;, y = &quot;Rawdata&quot;, x = &quot;compConcentration(nM)&quot;) + theme_minimal() #scatterplot of the data with fixed x-axis and jitter As we van see the x-axis is now readable. We see in the scatterplot that the positive control for this experiments is Ethanol. The negative control for this experiment is S-medium. if we really want to compare the different between the compund and experiments type. We need to normalize the data for the Negative control. We normalize the data by setting the mean value for the “controlNegative” to 1. All the other values are expressed as a fraction thereof. library(tidyverse) data_df &lt;- as.data.frame(data) controlnegative_mean &lt;- mean(data_df$RawData[data_df$compVehicle == &quot;controlNegative&quot;]) #calculate mean of the control negative data_df$data_norm &lt;- data_df$RawData/controlnegative_mean ggplot(data = data_df, aes(x = log10(compConcentration), y = data_norm)) + geom_jitter(aes(color = compName, shape = expType), size = 1.5,alpha = 0.8, width = 0.2) + labs(title = &quot;Rawdata of each compName and their expType&quot;, y = &quot;Rawdata normalized&quot;, x = &quot;compConcentration(nM)&quot;) + theme_minimal() #scatterplot of the data with fixed x-axis and jitter In the dataset we used for this analysis, each compound has his own concentrations. If we want make the data more comparable we have to normalize the data. 2.2 analyzing plan If we want to analyze the data to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve(ic50), First we need to get the data and prepare it. We did this already as you can see above. after preparing the data we will perform the shapiro.test for every compound to check if the data (Rawdata) is normally distributed. All compounds need to have a p-value greater than 0.05. To test if there is a significant difference between compound concentrations we can use the one sample t-test for each compound individually. To do this we have to filter the table for the compound we want to test first before we perform the test. To compare the IC50 of each compound we can use the {drc} package to visualize the different curves for each compound. "],["open-peer-review.html", "3 Open Peer Review 3.1 Introduction 3.2 Clarification 3.3 the second peer review", " 3 Open Peer Review 3.1 Introduction We’re gonna identify reproducibility issues in a scientific publication. We use the criteria for reproduciblity to see if our article is reproducible or not. The scientific article that we use can be found here. Table 3.1: Table1: The Transparency Criteria scores of the scientific article transparency Criteria Available Study Purpose Yes Data Availability Statement No Data Location No Study Location No Author Review Yes Ethics Statement No Funcing statement Yes Code Availability No 3.2 Clarification We’ve read the article and scored the article based on the Transparency Criteria in table 1. For each “Yes” in “Availability” the article scores one point. As we can see in table 1 the article scores four points out of eight. This article is not reproducible. The authors give us a statement why they started this research in the last paragraph. Although they show all their results, there is no raw data available of the western blot or the qPCR. The data of this study cant be accessed. There is no file or link with the raw or processed data available, only results. The paper doesn’t provide the exact study location in the methods section or else where. However it does give us a few laboratories where the research is done, like the Jackson Laboratory and the Bartek Laboratory. But since these laboratories have multiple locations we still don’t know where they performed the experiments. In the paper is an e-mail adress of one of the authors. There are no contact information of the other authors. The paper doesn’t give us a statement within the paper indicating any ethical concerns. The paper does give us a statement within the paper indicating that the authors received funding for their research. In the last paragraph under Results and Discussion the authors give use a summary of all their findings. The codes that the researchers used in this experiment are not available for the reader. 3.3 the second peer review h and i. https://osf.io/87mpk This code is analyzing two countries, the USA and India. This codes scores a two out of five on readability. the codes contain a ‘header’, but it could be more informative. A header like “Donation” doesn’t give the reader enough information. before we could run the code, we hade to download a dataset named “data.clean.txt”. library(rethinking) d&lt;-read.table(&quot;data.clean.txt&quot;,sep=&quot;\\t&quot;,header=T,stringsAsFactors = F) names(d) dat &lt;- list( C=d$CountC, A = standardize(d$Age), G = as.integer(as.factor(d$Gender)), #1 Man, 2 Woman CD = as.integer(d$COND), Mfm = standardize(d$MAC.fam), Mg = standardize(d$MAC.gro), Mr = standardize(d$MAC.rec), Mh = standardize(d$MAC.her), Md = standardize(d$MAC.def), Mfi = standardize(d$MAC.fai), Mp = standardize(d$MAC.pro), P = standardize(d$Precaution), S = standardize(d$Prosociality), D = standardize(d$donation), Dgr=standardize(d$danger.for.participant), CC = ifelse(d$COND==1,0,ifelse(d$COND==2,0,ifelse(d$COND==3,0,ifelse(d$COND==4,standardize(d$MAC.fam),ifelse(d$COND==5,standardize(d$MAC.gro),ifelse(d$COND==6,standardize(d$MAC.rec),ifelse(d$COND==7,standardize(d$MAC.her),ifelse(d$COND==8,standardize(d$MAC.def),ifelse(d$COND==9,standardize(d$MAC.fai),ifelse(d$COND==10,standardize(d$MAC.pro),NA)))))))))) #MAC dimension concordant with the condition ) set.seed(42) m1 &lt;- ulam( alist( D ~ dnorm(muD,sigmaD), muD&lt;-aG[G]+bA*A+bP*P+bS*S+aC[CD]+bCon*CC+bFam*Mfm+bGro*Mg+bRec*Mr+bHer*Mh+bDef*Md+bFai*Mfi+bPro*Mp+bDg*Dgr, #Donation aG[G]~dnorm(0,0.2), bA~dnorm(0,0.5), bP~dnorm(0,0.5), bS~dnorm(0,0.5), aC[CD]~dnorm(0,0.2), bCon~dnorm(0,0.5), bFam~dnorm(0,0.5), bGro~dnorm(0,0.5), bRec~dnorm(0,0.5), bHer~dnorm(0,0.5), bDef~dnorm(0,0.5), bFai~dnorm(0,0.5), bPro~dnorm(0,0.5), bDg~dnorm(0,0.5), #Model of precaution and prosociality P ~ dnorm(muP,sigmaP), S ~ dnorm(muS,sigmaS), muP&lt;-aGP[G]+bAP*A+aCP[CD]+bConP*CC+bFamP*Mfm+bGroP*Mg+bRecP*Mr+bHerP*Mh+bDefP*Md+bFaiP*Mfi+bProP*Mp+bDgP*Dgr, muS&lt;-aGS[G]+bAS*A+aCS[CD]+bConS*CC+bFamS*Mfm+bGroS*Mg+bRecS*Mr+bHerS*Mh+bDefS*Md+bFaiS*Mfi+bProS*Mp+bDgS*Dgr, #Priors #Precaution aGP[G]~dnorm(0,0.2), bAP~dnorm(0,0.5), aCP[CD]~dnorm(0,0.2), bConP~dnorm(0,0.5), bFamP~dnorm(0,0.5), bGroP~dnorm(0,0.5), bRecP~dnorm(0,0.5), bHerP~dnorm(0,0.5), bDefP~dnorm(0,0.5), bFaiP~dnorm(0,0.5), bProP~dnorm(0,0.5), bDgP~dnorm(0,0.5), #ProSociality aGS[G]~dnorm(0,0.2), bAS~dnorm(0,0.5), aCS[CD]~dnorm(0,0.2), bConS~dnorm(0,0.5), bFamS~dnorm(0,0.5), bGroS~dnorm(0,0.5), bRecS~dnorm(0,0.5), bHerS~dnorm(0,0.5), bDefS~dnorm(0,0.5), bFaiS~dnorm(0,0.5), bProS~dnorm(0,0.5), bDgS~dnorm(0,0.5), #sigmas sigmaD~dexp(1), sigmaP~dexp(1), sigmaS~dexp(1), #Models of MAC dimensions Mfm ~ dnorm(mu_Fam,sigma_Fam), Mg ~ dnorm(mu_Gro,sigma_Gro), Mr ~ dnorm(mu_Rec,sigma_Rec), Mh ~ dnorm(mu_Her,sigma_Her), Md ~ dnorm(mu_Def,sigma_Def), Mfi ~ dnorm(mu_Fai,sigma_Fai), Mp ~ dnorm(mu_Pro,sigma_Pro), mu_Fam&lt;-aG_Fam[G]+bAge_Fam*A, mu_Gro&lt;-aG_Gro[G]+bAge_Gro*A, mu_Rec&lt;-aG_Rec[G]+bAge_Rec*A, mu_Her&lt;-aG_Her[G]+bAge_Her*A, mu_Def&lt;-aG_Def[G]+bAge_Def*A, mu_Fai&lt;-aG_Fai[G]+bAge_Fai*A, mu_Pro&lt;-aG_Pro[G]+bAge_Pro*A, #priors of MAC intercepts and slopes aG_Fam[G]~dnorm(0,0.2), aG_Gro[G]~dnorm(0,0.2), aG_Rec[G]~dnorm(0,0.2), aG_Her[G]~dnorm(0,0.2), aG_Def[G]~dnorm(0,0.2), aG_Fai[G]~dnorm(0,0.2), aG_Pro[G]~dnorm(0,0.2), bAge_Fam~dnorm(0,0.5), bAge_Gro~dnorm(0,0.5), bAge_Rec~dnorm(0,0.5), bAge_Her~dnorm(0,0.5), bAge_Def~dnorm(0,0.5), bAge_Fai~dnorm(0,0.5), bAge_Pro~dnorm(0,0.5), #sigmas sigma_Fam~dexp(1), sigma_Gro~dexp(1), sigma_Rec~dexp(1), sigma_Her~dexp(1), sigma_Def~dexp(1), sigma_Fai~dexp(1), sigma_Pro~dexp(1), #model of how dangerous COVID is perceived for the participant Dgr ~ dnorm(mu_Dang,sigma_Dang), mu_Dang&lt;-aG_Dang[G]+bAge_Dang*A, #priors aG_Dang[G]~dnorm(0,0.2), bAge_Dang~dnorm(0,0.5), sigma_Dang~dexp(1) ) , data=dat, chains=4 , cores=4 , log_lik=TRUE ,iter = 5000,control=list(max_treedepth=10,adapt_delta=0.95)) #Sumarize the model precis(m1,depth=2) #Sample posetrior and prior for graphical comparison post1&lt;-extract.samples(m1) set.seed(42) prio1&lt;-extract.prior(m1,n=10000) save.image(file=&quot;posterior_samples_single.RData&quot;) While running the cod ewe come across a few problems and bugs we need to fix. the first problem is that R didn’t recognize rethinking as a package, I found a person on Stackoverflow with the same problem and he recommended me this: install.packages(&quot;rethinking&quot;, repos=c(cran=&quot;https://cloud.r-project.org&quot;, rethinking=&quot;http://xcelab.net/R&quot;)) The second problem was that Rstudio couldn’t find the function “standerdize”. A search on google gave me this:. if(!require(&#39;robustHD&#39;)) { install.packages(&#39;robustHD&#39;) library(&#39;robustHD&#39;) } The next problem is that R didn’t recognize the function “ulam”. This is weird since it is part of the “rethinking” package. i tried fixing it with the code below, but R doesn’t recognize “rethinking” as a package. R cant recognize this package because there is no version of this package available for the version of R that’s been used for this analyse. I tried something else and i found a quick installion for the ‘rethinking’ package. This worked and the function “ulam” got recognized. devtools::install_github(&quot;stan-dev/cmdstanr&quot;) install.packages(c(&quot;coda&quot;,&quot;mvtnorm&quot;,&quot;devtools&quot;,&quot;loo&quot;,&quot;dagitty&quot;)) devtools::install_github(&quot;rmcelreath/rethinking&quot;) The next struggle we come across is that the CmdStan path has not been set. To fix this we have to give R the location of the CmdStan installion. To find the location we use the “cmdstan_default_install_path()” function. After i set the path with set_cmdstan_path, R couldn’t find the directory. Unfortnetly, this problem i couln’t fix. library(rethinking) if(!require(&#39;robustHD&#39;)) { install.packages(&#39;robustHD&#39;) library(&#39;robustHD&#39;) } d&lt;-read.table(&quot;data.clean.txt&quot;,sep=&quot;\\t&quot;,header=T,stringsAsFactors = F) names(d) dat &lt;- list( C=d$CountC, A = standardize(d$Age), G = as.integer(as.factor(d$Gender)), #1 Man, 2 Woman CD = as.integer(d$COND), Mfm = standardize(d$MAC.fam), Mg = standardize(d$MAC.gro), Mr = standardize(d$MAC.rec), Mh = standardize(d$MAC.her), Md = standardize(d$MAC.def), Mfi = standardize(d$MAC.fai), Mp = standardize(d$MAC.pro), P = standardize(d$Precaution), S = standardize(d$Prosociality), D = standardize(d$donation), Dgr=standardize(d$danger.for.participant), CC = ifelse(d$COND==1,0,ifelse(d$COND==2,0,ifelse(d$COND==3,0,ifelse(d$COND==4,standardize(d$MAC.fam),ifelse(d$COND==5,standardize(d$MAC.gro),ifelse(d$COND==6,standardize(d$MAC.rec),ifelse(d$COND==7,standardize(d$MAC.her),ifelse(d$COND==8,standardize(d$MAC.def),ifelse(d$COND==9,standardize(d$MAC.fai),ifelse(d$COND==10,standardize(d$MAC.pro),NA)))))))))) #MAC dimension concordant with the condition ) set.seed(42) m1 &lt;- ulam( set_cmdstan_path(&quot;C:/Users/Lamya/OneDrive/Documenten/.cmdstan&quot;), alist( D ~ dnorm(muD,sigmaD), muD&lt;-aG[G]+bA*A+bP*P+bS*S+aC[CD]+bCon*CC+bFam*Mfm+bGro*Mg+bRec*Mr+bHer*Mh+bDef*Md+bFai*Mfi+bPro*Mp+bDg*Dgr, #Donation aG[G]~dnorm(0,0.2), bA~dnorm(0,0.5), bP~dnorm(0,0.5), bS~dnorm(0,0.5), aC[CD]~dnorm(0,0.2), bCon~dnorm(0,0.5), bFam~dnorm(0,0.5), bGro~dnorm(0,0.5), bRec~dnorm(0,0.5), bHer~dnorm(0,0.5), bDef~dnorm(0,0.5), bFai~dnorm(0,0.5), bPro~dnorm(0,0.5), bDg~dnorm(0,0.5), #Model of precaution and prosociality P ~ dnorm(muP,sigmaP), S ~ dnorm(muS,sigmaS), muP&lt;-aGP[G]+bAP*A+aCP[CD]+bConP*CC+bFamP*Mfm+bGroP*Mg+bRecP*Mr+bHerP*Mh+bDefP*Md+bFaiP*Mfi+bProP*Mp+bDgP*Dgr, muS&lt;-aGS[G]+bAS*A+aCS[CD]+bConS*CC+bFamS*Mfm+bGroS*Mg+bRecS*Mr+bHerS*Mh+bDefS*Md+bFaiS*Mfi+bProS*Mp+bDgS*Dgr, #Priors #Precaution aGP[G]~dnorm(0,0.2), bAP~dnorm(0,0.5), aCP[CD]~dnorm(0,0.2), bConP~dnorm(0,0.5), bFamP~dnorm(0,0.5), bGroP~dnorm(0,0.5), bRecP~dnorm(0,0.5), bHerP~dnorm(0,0.5), bDefP~dnorm(0,0.5), bFaiP~dnorm(0,0.5), bProP~dnorm(0,0.5), bDgP~dnorm(0,0.5), #ProSociality aGS[G]~dnorm(0,0.2), bAS~dnorm(0,0.5), aCS[CD]~dnorm(0,0.2), bConS~dnorm(0,0.5), bFamS~dnorm(0,0.5), bGroS~dnorm(0,0.5), bRecS~dnorm(0,0.5), bHerS~dnorm(0,0.5), bDefS~dnorm(0,0.5), bFaiS~dnorm(0,0.5), bProS~dnorm(0,0.5), bDgS~dnorm(0,0.5), #sigmas sigmaD~dexp(1), sigmaP~dexp(1), sigmaS~dexp(1), #Models of MAC dimensions Mfm ~ dnorm(mu_Fam,sigma_Fam), Mg ~ dnorm(mu_Gro,sigma_Gro), Mr ~ dnorm(mu_Rec,sigma_Rec), Mh ~ dnorm(mu_Her,sigma_Her), Md ~ dnorm(mu_Def,sigma_Def), Mfi ~ dnorm(mu_Fai,sigma_Fai), Mp ~ dnorm(mu_Pro,sigma_Pro), mu_Fam&lt;-aG_Fam[G]+bAge_Fam*A, mu_Gro&lt;-aG_Gro[G]+bAge_Gro*A, mu_Rec&lt;-aG_Rec[G]+bAge_Rec*A, mu_Her&lt;-aG_Her[G]+bAge_Her*A, mu_Def&lt;-aG_Def[G]+bAge_Def*A, mu_Fai&lt;-aG_Fai[G]+bAge_Fai*A, mu_Pro&lt;-aG_Pro[G]+bAge_Pro*A, #priors of MAC intercepts and slopes aG_Fam[G]~dnorm(0,0.2), aG_Gro[G]~dnorm(0,0.2), aG_Rec[G]~dnorm(0,0.2), aG_Her[G]~dnorm(0,0.2), aG_Def[G]~dnorm(0,0.2), aG_Fai[G]~dnorm(0,0.2), aG_Pro[G]~dnorm(0,0.2), bAge_Fam~dnorm(0,0.5), bAge_Gro~dnorm(0,0.5), bAge_Rec~dnorm(0,0.5), bAge_Her~dnorm(0,0.5), bAge_Def~dnorm(0,0.5), bAge_Fai~dnorm(0,0.5), bAge_Pro~dnorm(0,0.5), #sigmas sigma_Fam~dexp(1), sigma_Gro~dexp(1), sigma_Rec~dexp(1), sigma_Her~dexp(1), sigma_Def~dexp(1), sigma_Fai~dexp(1), sigma_Pro~dexp(1), #model of how dangerous COVID is perceived for the participant Dgr ~ dnorm(mu_Dang,sigma_Dang), mu_Dang&lt;-aG_Dang[G]+bAge_Dang*A, #priors aG_Dang[G]~dnorm(0,0.2), bAge_Dang~dnorm(0,0.5), sigma_Dang~dexp(1) ) , data=dat, chains=4 , cores=4 , log_lik=TRUE ,iter = 5000,control=list(max_treedepth=10,adapt_delta=0.95)) #Sumarize the model precis(m1,depth=2) #Sample posetrior and prior for graphical comparison post1&lt;-extract.samples(m1) set.seed(42) prio1&lt;-extract.prior(m1,n=10000) save.image(file=&quot;posterior_samples_single.RData&quot;) From a scale from 1 (very hard) to 5(very easy). This script gets a 1. It was very hard to read the code without any pseudo coding. It’s hard to see what the author wants to achieve with this code. Running the code wasn’t an easy job either. A lot of packages needed to be installed in a different way than what the author did. In conclusion, this code is not really reproducible. "],["the-guerrila-structure.html", "4 The Guerrila structure 4.1 Why organizing data?", " 4 The Guerrila structure 4.1 Why organizing data? Organizing data is crucial for reproducibility and a good workflow in general. Below us we find an example of a folder containing multiple projects i made. This dataset is orginized by the Guerrilla Analytics Principles. Each project contains a folder “data” and “output”. And each folder contains a “README” file about the folder and data. Each “data” folder contains a “rawdata” folder. In this folder is the file with the rawdata. This data will not be used during the experiment, instead we have a copy of the dataset in “data” to prevent errors and false data from happening. ## C:/Users/Lamya/OneDrive/Documenten/dsfb2_workflows_portfolio/portofolio_bookdown/data/Guerrila_structure ## ├── metagenomics ## │ ├── metagenomics_formatief ## │ │ ├── data ## │ │ │ ├── HU2_MOCK2_L001_R1_001.fastq.gz ## │ │ │ ├── HU2_MOCK2_L001_R2_001.fastq.gz ## │ │ │ ├── HU_waternet_MOCK2_composition_copy.csv ## │ │ │ ├── rawdata ## │ │ │ │ └── HU_waternet_MOCK2_composition.csv ## │ │ │ └── README.txt ## │ │ ├── output ## │ │ │ ├── metagenomics_formatief.html ## │ │ │ └── metagenomics_formatief.Rmd ## │ │ └── README.txt ## │ ├── metagenomics_reader ## │ │ ├── data ## │ │ │ ├── HU1_MOCK1_L001_R1_001.fastq.gz ## │ │ │ ├── HU1_MOCK1_L001_R2_001.fastq.gz ## │ │ │ ├── HU_waternet_MOCK1_composition_copy.csv ## │ │ │ ├── rawdata ## │ │ │ │ ├── HU_waternet_MOCK1_composition.csv ## │ │ │ │ └── README.txt ## │ │ │ └── README.txt ## │ │ ├── output ## │ │ │ ├── metagenomics_reader.html ## │ │ │ └── metagenomics_reader.Rmd ## │ │ └── README.txt ## │ └── README.txt ## └── rna_sequencing ## ├── README.txt ## ├── rnaseq_airway ## │ ├── data ## │ │ ├── airway_sampledata_copy.csv ## │ │ ├── bam ## │ │ │ ├── READMe.txt ## │ │ │ ├── SRR1039508.bam ## │ │ │ ├── SRR1039508.bam.indel.vcf ## │ │ │ └── SRR1039508.bam.summary ## │ │ ├── counts ## │ │ │ ├── README.txt ## │ │ │ └── read_counts.rds ## │ │ ├── fastq ## │ │ │ ├── SRR1039516_1.fastq.gz ## │ │ │ ├── SRR1039516_2.fastq.gz ## │ │ │ ├── SRR1039517_1.fastq.gz ## │ │ │ └── SRR1039520_1.fastq.gz ## │ │ ├── rawdata ## │ │ │ └── airway_sampledata.csv ## │ │ └── README.txt ## │ ├── output ## │ │ └── fastqc_output ## │ │ ├── SRR1039508_1_fastqc.html ## │ │ ├── SRR1039508_1_fastqc.zip ## │ │ ├── SRR1039508_2_fastqc.html ## │ │ └── SRR1039508_2_fastqc.zip ## │ └── README.txt ## ├── rnaseq_ipsc ## │ ├── data ## │ │ ├── bam ## │ │ │ ├── SRR7866687.bam ## │ │ │ ├── SRR7866687.bam.indel.vcf ## │ │ │ └── SRR7866687.bam.summary ## │ │ ├── counts ## │ │ │ └── read_counts.rds ## │ │ ├── fastq ## │ │ │ ├── SRR7866693_1.fastq.gz ## │ │ │ ├── SRR7866693_2.fastq.gz ## │ │ │ ├── SRR7866694_1.fastq.gz ## │ │ │ └── SRR7866694_2.fastq.gz ## │ │ ├── ipsc_sampledata_copy.csv ## │ │ ├── rawdata ## │ │ │ └── ipsc_sampledata.csv ## │ │ └── README.txt ## │ ├── output ## │ │ ├── fastqc_output ## │ │ │ ├── README.txt ## │ │ │ ├── SRR7866687_1_fastqc.html ## │ │ │ ├── SRR7866687_1_fastqc.zip ## │ │ │ ├── SRR7866687_2_fastqc.html ## │ │ │ └── SRR7866687_2_fastqc.zip ## │ │ └── README.txt ## │ └── README.txt ## └── rnaseq_onecut ## ├── data ## │ ├── bam ## │ │ ├── SRR7866699.bam ## │ │ ├── SRR7866699.bam.indel.vcf ## │ │ └── SRR7866699.bam.summary ## │ ├── counts ## │ │ └── read_counts_OC3.rds ## │ ├── fastqc ## │ │ ├── SRR7866703_1.fastq.gz ## │ │ ├── SRR7866703_2.fastq.gz ## │ │ ├── SRR7866704_1.fastq.gz ## │ │ ├── SRR7866704_2.fastq.gz ## │ │ ├── SRR7866705_1.fastq.gz ## │ │ └── SRR7866706_2.fastq.gz ## │ ├── onecut_sampledata_OC3_copy.csv ## │ ├── rawdata ## │ │ └── onecut_sampledata_OC3.csv ## │ └── README.txt ## ├── output ## └── README.txt "],["western-blot-analysis-using-rstudio.html", "5 Western Blot analysis using Rstudio 5.1 Introduction 5.2 planning", " 5 Western Blot analysis using Rstudio 5.1 Introduction Western Blotting (WB) is a commonly used method in the biological scienes. This technique is used to investigate many features of the protein, ranging from basic protein analysis to disease detection such as cancer and HIV (https://www.future-science.com/doi/10.2144/btn-2022-0003). Western Blotting involves three components to achieve this goal: (1) separating based on size, (2) transferring onto a solid surface, and (3) labeling the protein of interest with specific primary and secondary antibodies for visualization (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3456489/). This results in a gel full of different proteins separated by size. ImageJ is often used to analyze the Western Blot results. A lane is The technique is highly sensitive and can be used to detect even small amounts of protein. It is often used in research into diseases such as cancer and HIV, where specific proteins can be used as markers for diagnosis and treatment. The next skill im gonna learn is to analyse Western Blots using Rstudio. 5.2 planning The first thing i need to do is to read about Western blot analysis in Rstudio. I need to look if people made packages and if they succeed. Most of the Western Blot analysis is done with ImageJ, which makes my search for Rcodes a bit difficultier. Find articles about photo analysis and Western blot analysis in Rstudio. Look for codes that analysist used for Western Blot results. Try codes on Western blot photo’s. "],["relational-databases.html", "6 Relational databases", " 6 Relational databases First we gonna load in the datasets into three separated dataframes in R. #load in required packages library(tidyverse) library(dslabs) #load in gapminder dataset as dataframe gapminder_data &lt;- as.data.frame(gapminder) head(gapminder_data) #show the first 12 rows of the dataframe ## country year infant_mortality life_expectancy fertility ## 1 Albania 1960 115.40 62.87 6.19 ## 2 Algeria 1960 148.20 47.50 7.65 ## 3 Angola 1960 208.00 35.98 7.32 ## 4 Antigua and Barbuda 1960 NA 62.97 4.43 ## 5 Argentina 1960 59.87 65.39 3.11 ## 6 Armenia 1960 NA 66.86 4.55 ## population gdp continent region ## 1 1636054 NA Europe Southern Europe ## 2 11124892 13828152297 Africa Northern Africa ## 3 5270844 NA Africa Middle Africa ## 4 54681 NA Americas Caribbean ## 5 20619075 108322326649 Americas South America ## 6 1867396 NA Asia Western Asia #load in dengue dataset dengue_data &lt;- read.csv(&quot;./data/raw_data/dengue_data.csv&quot;, skip = 10) dengue_df &lt;- as.data.frame(dengue_data) #make dataframe head(dengue_df) #show the first 12 rows of the dataframe ## Date Argentina Bolivia Brazil India Indonesia Mexico Philippines ## 1 2002-12-29 NA 0.101 0.073 0.062 0.101 NA NA ## 2 2003-01-05 NA 0.143 0.098 0.047 0.039 NA NA ## 3 2003-01-12 NA 0.176 0.119 0.051 0.059 0.071 NA ## 4 2003-01-19 NA 0.173 0.170 0.032 0.039 0.052 NA ## 5 2003-01-26 NA 0.146 0.138 0.040 0.112 0.048 NA ## 6 2003-02-02 NA 0.160 0.202 0.038 0.049 0.041 NA ## Singapore Thailand Venezuela ## 1 0.059 NA NA ## 2 0.059 NA NA ## 3 0.238 NA NA ## 4 0.175 NA NA ## 5 0.164 NA NA ## 6 0.163 NA NA #load in flu dataset flu_data &lt;- read.csv(&quot;./data/raw_data/flu_data.csv&quot;, skip = 10) flu_df &lt;- as.data.frame(flu_data) #make datafram of flu_data head(flu_df) #show the first 12 rows of the dataframe ## Date Argentina Australia Austria Belgium Bolivia Brazil Bulgaria Canada ## 1 2002-12-29 NA NA NA NA NA 174 NA NA ## 2 2003-01-05 NA NA NA NA NA 162 NA NA ## 3 2003-01-12 NA NA NA NA NA 174 NA NA ## 4 2003-01-19 NA NA NA NA NA 162 NA NA ## 5 2003-01-26 NA NA NA NA NA 131 NA NA ## 6 2003-02-02 136 NA NA NA NA 151 NA NA ## Chile France Germany Hungary Japan Mexico Netherlands New.Zealand Norway ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 1 NA NA NA NA NA NA NA NA ## 4 0 NA NA NA NA NA NA NA NA ## 5 0 NA NA NA NA NA NA NA NA ## 6 0 NA NA NA NA NA NA NA NA ## Paraguay Peru Poland Romania Russia South.Africa Spain Sweden Switzerland ## 1 NA 329 NA NA NA NA NA NA NA ## 2 NA 315 NA NA NA NA NA NA NA ## 3 NA 314 NA NA NA NA NA NA NA ## 4 NA 267 NA NA NA NA NA NA NA ## 5 NA 241 NA NA NA NA NA NA NA ## 6 NA 227 NA NA NA NA NA NA NA ## Ukraine United.States Uruguay ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA In the tibbles above us we see the gapminder, flu and the dengue dataframes. The gapminder dataframe is tidy, there is only one observation in each row The flu dataframe and the dengue dataframe are not tidy as we can see in the dataframes above us. There are more than one observation in each row. To make both dataframes tidy we use {privot_longer}. #make dengue dataframe tidy using &#39;privot_longer&#39; dengue_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;dengue_cases&quot;) #show the first 12 rows of the dataset head(dengue_tidy) ## # A tibble: 6 × 3 ## Date country dengue_cases ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Bolivia 0.101 ## 3 2002-12-29 Brazil 0.073 ## 4 2002-12-29 India 0.062 ## 5 2002-12-29 Indonesia 0.101 ## 6 2002-12-29 Mexico NA #make flu dataframe tidy using &#39;privot_longer&#39; flu_tidy&lt;- flu_df %&gt;% pivot_longer(cols = Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;flu_cases&quot;) #show the first 12 rows of the dataset head(flu_tidy) ## # A tibble: 6 × 3 ## Date country flu_cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Australia NA ## 3 2002-12-29 Austria NA ## 4 2002-12-29 Belgium NA ## 5 2002-12-29 Bolivia NA ## 6 2002-12-29 Brazil 174 #show the first 12 rows of the dataset head(gapminder) ## country year infant_mortality life_expectancy fertility ## 1 Albania 1960 115.40 62.87 6.19 ## 2 Algeria 1960 148.20 47.50 7.65 ## 3 Angola 1960 208.00 35.98 7.32 ## 4 Antigua and Barbuda 1960 NA 62.97 4.43 ## 5 Argentina 1960 59.87 65.39 3.11 ## 6 Armenia 1960 NA 66.86 4.55 ## population gdp continent region ## 1 1636054 NA Europe Southern Europe ## 2 11124892 13828152297 Africa Northern Africa ## 3 5270844 NA Africa Middle Africa ## 4 54681 NA Americas Caribbean ## 5 20619075 108322326649 Americas South America ## 6 1867396 NA Asia Western Asia All three datasets are now tidy. Each row contains their own observation. If we compare the colomn type of the flu and dengue dataset, we can see some difference in these datasets. The colomns ‘year’ and ‘country’ dont have the same variables and data type as the gapminder dataset. We need to change these colomn so they match with each other. library(tidyverse) dengue_tidy$country &lt;- as.factor(dengue_tidy$country) #transform data type to factor dengue_tidy &lt;- dengue_tidy %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), convert = T, sep = &quot;-&quot;) #separate &#39;date&#39; into &#39;year&#39;, &#39;month&#39; and &#39;day&#39; flu_tidy$country &lt;- as.factor(flu_tidy$country) #transform data type to factor flu_tidy &lt;- flu_tidy %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), convert = T, sep = &quot;-&quot;) #separate &#39;date&#39; into &#39;year&#39;, &#39;month&#39; and &#39;day&#39; dengue_tidy %&gt;% head(5) ## # A tibble: 5 × 5 ## year month day country dengue_cases ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Bolivia 0.101 ## 3 2002 12 29 Brazil 0.073 ## 4 2002 12 29 India 0.062 ## 5 2002 12 29 Indonesia 0.101 flu_tidy %&gt;% head(5) ## # A tibble: 5 × 5 ## year month day country flu_cases ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Australia NA ## 3 2002 12 29 Austria NA ## 4 2002 12 29 Belgium NA ## 5 2002 12 29 Bolivia NA We’re now gonna store each tables we created as .csv and .rds files. #store dataset as .csv file dengue_tidy %&gt;% write.csv(&quot;data/denguetidy.csv&quot;) flu_tidy %&gt;% write.csv(&quot;data/flutidy.csv&quot;) gapminder %&gt;% write.csv(&quot;data/gapminder.csv&quot;) #store dataset as .rds file dengue_tidy %&gt;% write_rds(&quot;data/denguetidy.rds&quot;) flu_tidy %&gt;% write_rds(&quot;data/flutidy.rds&quot;) gapminder %&gt;% write_rds(&quot;data/gapminder.rds&quot;) For the next part of this analyse, we need a few packages to use Dbeaver and PostgreSQL. We downloaded the following packages. install.packages(&#39;RPostgreSQL&#39;) #install.packages(&#39;devtools&#39;) # you should have this one already install.packages(&#39;remotes&#39;) install.packages(&#39;RPostgres&#39;) We have created a new databse called ‘workflowsdb’ in Dbeaver. To do this right click on postgres &gt; SQL Editor &gt; Open SQL console. In the console we created a new database with the following code: CREATE DATABASE workflowsdb. To insert these three datasets, we need to connect to the workflows database we just created. Once we are connected, we’re gonna insert the tables into the database. #load in DBI package library(DBI) #connect to Dbeaver workflows database con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;password&quot;) #Make tables dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) dbWriteTable(con, &quot;flu&quot;, flu_tidy) dbWriteTable(con, &quot;gapminder&quot;, gapminder_data) We need to know if everything is imported correctly. We need to inspect the contents of the tables with SQL and in R. #load in DBI package library(DBI) #er moet nog een screen komen van de dataset in Dbeaver!!!!!!!!! #inspect if datasets are corrrectly imported dengue_s &lt;- dbReadTable(con, &quot;dengue&quot;) flu_s &lt;- dbReadTable(con, &quot;flu&quot;) gapminder_s &lt;- dbReadTable(con, &quot;gapminder&quot;) dengue_s %&gt;% head(5) ## year month day country dengue_cases ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Bolivia 0.101 ## 3 2002 12 29 Brazil 0.073 ## 4 2002 12 29 India 0.062 ## 5 2002 12 29 Indonesia 0.101 flu_s %&gt;% head(5) ## year month day country flu_cases ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Australia NA ## 3 2002 12 29 Austria NA ## 4 2002 12 29 Belgium NA ## 5 2002 12 29 Bolivia NA gapminder_s %&gt;% head(5) ## country year infant_mortality life_expectancy fertility ## 1 Albania 2002 21.0 75.5 2.20 ## 2 Algeria 2002 32.4 73.8 2.41 ## 3 Angola 2002 125.5 53.3 6.78 ## 4 Antigua and Barbuda 2002 12.3 74.3 2.29 ## 5 Argentina 2002 17.1 74.3 2.38 ## population gdp continent region ## 1 3123112 4059111575 Europe Southern Europe ## 2 31990387 58856686557 Africa Northern Africa ## 3 16109696 10780448534 Africa Middle Africa ## 4 80030 840972692 Americas Caribbean ## 5 37889443 242076212334 Americas South America We now want to join these 3 databases. To join these three datasets together we need to clean the gapminder data so it could join the dengue and flue dataset. We saw that all 3 datasets have a colomn called ‘year’ and ‘country’. We already changed the colomn ‘year’ and ‘country’ in the flu en dengue dataset to match the gapminder dataset. First we need to check their ranges in the colomn ‘year’. What’s their minimal year and what is their max year and how many years does each dataset contain? #check the ranges of each dataset and put them in a list list(gapminder_data$year %&gt;% unique(), dengue_tidy$year %&gt;% unique(), flu_tidy$year %&gt;% unique()) ## [[1]] ## [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 ## [16] 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 ## [31] 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 ## [46] 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 ## ## [[2]] ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 ## ## [[3]] ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 As we can see, the gapminder has a wider range than the flu and dengue datasets. We’ll remove the years that aren’t in the flu and dengue dataset so we could join these three datasets together. #remove the years that the flu and dengue dataset not contain gapminder_data &lt;- gapminder %&gt;% subset(year %in% c(2002:2015) ) gapminder_data$year %&gt;% unique() ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 The gapminder dataset has now the same range as the flu and dengue dataset. we’ll remove the old gapminder dataset for the new clean dataset named “gapminder_data” in Dbeaver with SQL. #replace the old gapminder dataset with the new gapminder dataset dbRemoveTable(con, &quot;gapminder&quot;) dbWriteTable(con, &quot;gapminder&quot;, gapminder_data) #laat zien dat het ook is gebeurd!!!! We can now join all three datasets together in DBeaver with SQL. we’re using inner_join to make a dataset with variables that are available in both datasets. we’re using inner_join to make a dataset with variables that are available in both datasets. #inner join the flu and dengue dataset flu_dengue &lt;- inner_join( flu_tidy, dengue_tidy, by = c(&quot;country&quot;, &quot;year&quot;) ) #inner join the gapminder dataset with flu_dengue gapminder_dengue_flu &lt;- inner_join( gapminder_data, flu_dengue, by = c(&quot;country&quot;, &quot;year&quot;) ) #left join gapminder with flu gapminder_flu &lt;- left_join( gapminder_data, flu_tidy, by = c(&quot;country&quot;, &quot;year&quot;) ) #left join gapminder with dengue gapminder_flu &lt;- left_join( gapminder_data, dengue_tidy, by = c(&quot;country&quot;, &quot;year&quot;) ) gapminder_dengue_flu %&gt;% ggplot() + geom_col(aes(x = country, y = flu_cases, fill = country) ) + theme(axis.text.x = element_text(angle = 25), legend.position = &quot;none&quot; ) + labs(title = &quot;Flu cases per country&quot;, subtitle = &quot;flu cases per country around the world in total&quot;, x = &quot;country&quot;, y = &quot;flu cases&quot; ) gapminder_dengue_flu %&gt;% ggplot() + geom_col(aes(x = country, y = dengue_cases, fill = country) ) + theme(axis.text.x = element_text(angle = 25), legend.position = &quot;none&quot; ) + labs(title = &quot;Dengue cases per country&quot;, subtitle = &quot;dengue cases per country around the world in total&quot;, x = &quot;country&quot;, y = &quot;dengue cases&quot; ) flu_dengue_2003 &lt;- flu_dengue %&gt;% filter(year==2003) correlation&lt;-cor.test(flu_dengue_2003$flu_cases, flu_dengue_2003$dengue_cases, method=c(&quot;pearson&quot;))$estimate %&gt;% round(digits = 3) ggplot(data = flu_dengue_2003, aes(x = flu_cases, y = dengue_cases)) + geom_point(aes(color = country, shape = country), size = 0.8,alpha = 0.8, width = 0.2)+ labs(title = &quot;Relation between flu cases and dengue cases&quot;, y = &quot;dengue cases&quot;, x = &quot;flu cases&quot;) + theme_minimal() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
